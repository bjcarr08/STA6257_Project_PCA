---
title: "PCA Slides"
aliases: 
  - slides.html#PCA-Slides
author: 
- Brandy Carr
- Lee
- Tavish
format: 
  revealjs:
    theme: [default, slides_files/libs/revealjs/custom.scss]
    scrollable: true
    smaller: true
    margin: 0.01
    auto-stretch: false
    width: 1200
    max-scale: 1.5
    view-distance: 30
    mobile-view-distance: 30
editor: visual
bibliography: references.bib # file contains bibtex for references
course: STA 6257 - Advance Statistical Modeling
---

## What is it? {background-color="#FAFAF5"}

-   Principal component analysis (PCA) is an unsupervised machine learning method that can be used as the first step in data analysis
-   Main goal: reduce the number of dimensions/features while retaining most of the original variability
-   This is achieved using a vector space transform

## Machine Learning Techniques {auto-animate="true" auto-animate-easing="ease-in-out" background-color="#FAFAF5" color="#8B814C"}

![](index_files/figure-html/machineLearning_fig0.png){#fig-machineLearning data-id="fig1" height="520px" width="584px" fig-align="center"}

## Machine Learning Techniques {auto-animate="true" auto-animate-easing="ease-in-out" background-color="#FAFAF5" color="#8B814C"}

![](index_files/figure-html/machineLearning_fig1.png){data-id="fig1" height="520px" width="584px" fig-align="center"}

## Why use PCA? {background-color="#FAFAF5"}

<br>

-   Makes it easier to graphically visualize hidden patterns in the data
-   High number of predictor variables or highly correlated variables when running a regression model
-   To observe hidden trends, jumps, clusters, patterns and outliers
-   PCA assumes no distribution in data in a descriptive context
-   Widely adaptive as an exploratory method across disciplines and data types

## Where has PCA been applied? {background-color="#FAFAF5"}

<br>

-   PCA is a broadly used statistical method whose use stretches across many scientific disciplines
-   Image analysis, analysis of web data, cyber security analysis, mental disorders, recognizing microanerysm, facial recognition, etc.
-   Many different adaptations of PCA have been developed based on the variation in goals and data types associated with each respective discipline

## How does it work? {background-color="#FAFAF5"}

<br>

-   By mathematical projection we can interpret the original data set with just a few variables, namely the principal components
-   The process of reduction preserves the maximum variability in the data (i.e. the statistical information present in the data)
-   "...find new variables that are linear functions of those in the original data set, that successively maximize variance and that are uncorrelated with each other" [@Jolliffe]

## Methods {background-color="#FAFAF5"}

PCA is an unsupervised machine learning tool used to find hidden patterns in multivariate datasets.

It is often used as the 1st step when preforming other multivariate methods such as: 

- multiple regression
- cluster analysis
- discriminant analysis

The main goal is to take the original correlated variables and project them into a new set of smaller uncorrelated variables, the principal components. (dimension reduction)

PCA is the most beneficial when the first 2 PC's (combined) explain over 80% of the variance. 

Most often only the first 2 PC's are of importance because it is easiest te represent visually, with the 1st PC as the xaxis, the 2nd as the yaxis, and the eigenvectors (loadings) represented as arrows originating from the center of the plot.  


## Assumptions & Process {#sec-assumptions background-color="#FAFAF5"}

1. Variables should be continuous, interval or ratio level of measurement (ordinal variables, such as likert-scale, are also often used)
2. Variables should all be the same scale/units & if not, they should be standardized
3. Variables are linearly related
4. Outliers & missing or impossible values should be removed

```{mermaid, code-fold: hide}

%%{init: {'theme': 'base', 'themeVariables': {'mainBkg': '#FAFAF5', 'background': '#FAFAF5', 'primaryColor': '#EAEAD6', 'nodeBorder': '#8B814C', 'lineColor': '#8B814C', 'primaryTextColor': '#191970', 'textColor': '#191970', 'fontSize': '12px', 'width': '100%'}}}%%

flowchart LR
A[(Clean<br/>Data)] --> B((Are all<br/>vars the same<br/>scale/unit?))
B --> C((Yes))
B --> D((No))
D -.- |Standardize<br/>Data| E(Estimate<br/>Sample<br/>Mean<br/>Vector)
C --> E
D --> F(Estimate<br/>Sample<br/>Mean<br/>Vector)
E --> G(Estimate<br/>Sample<br/>Covariance<br/>Matrix)
F --> H(Estimate<br/>Sample<br/>Correlation<br/>Matrix)
G --> I(Eigenvalues<br/>Eigenvectors)
H --> I

```

## Sample data {background-color="#FAFAF5"}

-   COLUMNS: Variables
-   ROWS: Observations

![Sample Data](slides_files/libs/revealjs/tbl_sampleData.PNG){#fig-sampleData data-id="fig3" height="470px" width="900px" fig-align="left"}

## Equations {.smaller background-color="#FAFAF5"}

$${\sf Standardize\ Data:}\ \ \ \ \ \ \ \ {z_{ij}}\ =\ \frac{{x_{ij}} - {\overline{x_j}}} {\sqrt{\hat{\sigma_{jj}}}}, \ \ \ \ \ where\ \ \ \ i\ =\ \{1,2,...,n\}, \ \ \ \ j\ =\ \{1,2,...,p\}$$ {#eq-stdzData}

$${\sf Random\ Vector:}\ \ \ \ \ \ \ \ {x_i} = {({x_{i1}}, ... , {x_{ip}})}^T,\ \ \ \ \ \ \ \ \ where\ \ i\ =\ \{1,2,...,n\}$$ {#eq-randomVector}

$${\sf Sample\ Mean\ Vector:}\ \ \ \ \ \ \ \ \hat{\mu}\ \ =\ \ \overline{x}\ \ =\ \ \frac{1}{n} \sum_{i=1}^{n} x_i$$ {#eq-meanVector}

$${\sf Sample\ Covariance\ Vector:}\ \ \ \ \ \ \ \ \hat{\sum}\ \ =\ \ S\ \ =\ \ {(\hat{\sigma}_{ij})}_{p{\sf x}p}\ \ =\ \ \frac{1}{n-1} \sum_{i=1}^{n} {(x_i - \overline{x})(x_i - \overline{x})}^T$$ {#eq-covMatrix}

$${\sf Eigenvectors:}\ \ \ \ \ \ \ \ {\hat{a}_{k}}$$ {#eq-eigenvec}

$${\sf Eigenvalues:}\ \ \ \ \ \ \ \ {\hat{y}_{ik}}\ =\ {\hat{a}^T_{k}}(x_i\ -\ \overline{x}),\ \ \ \ \ \ \ i\ =\ \{1,2,...,n\},\ \ \ \ \ \ \ k\ =\ \{1,2,...,p\}$$ {#eq-eigenval}

## Data {background-color="#FAFAF5"}

[Replication Data & Code](https://thedata.harvard.edu/dvn/dv/ajps)

[@originalConspiracyData]

::: panel-tabset
### Description

-   2,000 respondents answering a 5-point Likert scale on their belief in various conspiracies
-   Grouped by political ideology
-   Sampled from a nationally representative survey in 2011

### Groups (clusters)

-   Very Liberal
-   Liberal
-   Somewhat Liberal
-   Middle of the Road
-   Somewhat Conservative
-   Conservative
-   Very Conservative

### Variables {.smaller}

All variables were measured using a 5-point likert scale: $$Strongly\ Disagree\ (1)\ <\ Disagree\ (2)\ <\ Neutral\ (3)\ <\ Agree\ (4)\ <\ Strongly\ Agree\ (5)$$

1.   **truther911:**  Certain U.S. government officials planned the attacks of September 11, 2001, to incite war
2.   **obamabirth:**  President Barack Obama was not really born in the US and does not have an authentic Hawaiian birth certificate
3.   **fincrisis:**  The current financial crisis was secretly orchestrated by a small group of Wall Street bankers to extend the power of the Federal Reserve and further their control of the world's economy
4.   **flourolights:**  The U.S. government is mandating the switch to compact fluorescent light bulbs because such lights make people more obedient and easier to control
5.   **endtimes:**  We are currently living in End Times as foretold by Biblical prophecy
6.   **sorosplot:**  Billionaire George Soros is behind a hidden plot to destabilize the American government, take control of the media, and put the world under his control
7.   **iraqjews:**  The U.S. invasion of Iraq was driven by oil companies and Jews in the U.S. and Israel
8.   **vaportrail:**  Vapor trails left by aircraft are actually chemical agents deliberately sprayed in a clandestine program directed by government officials

:::

## Check Assumptions {background-color="#FAFAF5"}

::: panel-tabset

### Scales/Units

![](index_files/figure-html/plot1-1.png){#fig-rawDistributions}
### Linearity

![](index_files/figure-html/plot0-1.png){#fig-linearity}

:::

## Screeplot {background-color="#FAFAF5"}

![](index_files/figure-html/plot2-1.png){#fig-Screeplot height="400px"}

![](slides_files/libs/revealjs/tbl_screeplot.PNG){#fig-tblScreeplot}


## PCA Biplot with Political Ideology Clusters {background-color="#FAFAF5"}

::: {layout="[35,65]" layout-valign="center"}

![](slides_files/libs/revealjs/tbl_biplot.PNG){#fig-tblBiblot}

![](index_files/figure-html/plot3-1.png){#fig-biplot}

:::

## Good Examples {background-color="#FAFAF5"}

::: {layout="[32,68]" layout-valign="center"}
![Good Example 1 [@goodEx1]](slides_files/libs/revealjs/pca_goodExample1.jpg){#fig-goodEx1}

![Good Example 2 [@goodEx2]](slides_files/libs/revealjs/pca_goodExample2.jpeg){#fig-goodEx2}
:::

## Bad Examples {background-color="#FAFAF5"}

::: {layout="[1, 1]" layout-valign="center"}
![Bad Example 1 [@badEx1]](slides_files/libs/revealjs/badExample1.png){#fig-badEx1}

![Bad Example 2 [@badEx2]](slides_files/libs/revealjs/000024.png){#fig-badEx2}
:::

## Conclusion {background-color="#FAFAF5"}

-   PCA is a very powerful tool for data analysis. It allows us to see patterns and trends hidden in large sets of data that would otherwise be very difficult to make out.

. . .

-   In our conclusion of Data, we saw that Scree-plot Proportion of Variance 0.4256 0.2027 0.09081 0.0669 0.06306 0.05966 0.05214 0.03907 and as well that plot dropped 10% from variances after Comp 3.

. . .

-   We focus on the Assumptions we ensure the variables are continuous, intervals or ratio levels of measurements are also of the same scale and units.

. . .

-   We also make sure in our dataset that the variables are linearly related, and we use the scatterplot to check the variables. Lastly, we ensure Outliers are removed and those Null values that are missing are noted and removed

. . .

-   PCA has been found to be useful when performing k-means clustering which by itself is a clustering method with countless applications. With all of the flexibility and usefulness of PCA taken into consideration it is easy to see why it is such a popular way to analyze large data sets across so many fields.

. . .

-   As long as there are large data sets there will be a demand to reduce the dimension of that data and make valid interpretations about the trends present in that data, to this end we have PCA.

## End {background-color="#FAFAF5"}
