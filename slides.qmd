---
title: "Prinicpal Component Analysis"
author: 
- Brandy Carr
- Lee
- Tavish
format: 
  revealjs:
    theme: dark
editor: visual
bibliography: references.bib # file contains bibtex for references
---

## What is it?

-   Principal component analysis (PCA) is an unsupervised machine learning method that can be used as the first step in data analysis
-   Main goal: reduce the number of dimensions/features while retaining most of the original variability
-   This is achieved using a vector space transform

## Figure

![Machine Learning](index_files/figure-html/machineLearning_fig1.png){#fig-machineLearning}


## Why use PCA?

-   Makes it easier to graphically visualize hidden patterns in the data
-   High number of predictor variables or highly correlated variables when running a regression model
-   To observe hidden trends, jumps, clusters, patterns and outliers
-   PCA assumes no distribution in data in a descriptive context
-   Widely adaptive as an exploratory method across disciplines and data types

## Where has PCA been applied?

-   PCA is a broadly used statistical method whose use stretches across many scientific disciplines
-   Image analysis, analysis of web data, cyber security analysis, mental disorders, recognizing microanerysm, facial recognition, etc.
-   Many different adaptations of PCA have been developed based on the variation in goals and data types associated with each respective discipline

## How does it work?

-   By mathematical projection we can interpret the original data set with just a few variables, namely the principal components
-   The process of reduction preserves the maximum variability in the data (i.e. the statistical information present in the data)
-   "...find new variables that are linear functions of those in the original data set, that successively maximize variance and that are uncorrelated with each other" [@Jolliffe]

## Methods

- PCA forms the basis of multivariate data analysis based on projection methods
- Variability in the original (correlated) variables is explained through a 
smaller (uncorrelated) set of new
variables i.e. the principal components (PC's)
- For unscaled data calculations should either be preformed on the correlation matrix
(instead of the covariance matrix)
- the data should be standardized with mean 0 and variance 1 (z scores)

## Methods continued ...

- A new set of orthogonal coordinate axes are  identified from the original data set
- Accomplished by finding the direction of maximal variance through each dimension
- Equivalent to using the least squares method
- This new axis is the first principal component of the data set

## Methods continued ...

- Next, orthogonal projection is used to project the coordinates onto the new axis
- Obtain a second principal component (and principal coordinate axis) by finding the direction of the second largest variance in the data, orthogonal to our first PC
- These two PCs define a plane onto which we can project further coordinates onto

## Methods figure

```{mermaid, code-fold: hide}

%%{init: {'theme': 'base', 'themeVariables': { 'background': '#FAFAF5', 'primaryColor': '#EAEAD6', 'nodeBorder': '#8B814C', 'lineColor': '#8B814C', 'primaryTextColor': '#191970', 'textColor': '#191970', 'fontSize': '12px', 'width': '100%'}}}%%

flowchart LR
A[(Clean<br/>Data)] --> B((Are all<br/>vars the same<br/>scale/unit?))
B --> C((Yes))
B --> D((No))
D -.- |Standardize<br/>Data| E(Estimate<br/>Sample<br/>Mean<br/>Vector)
C --> E
D --> F(Estimate<br/>Sample<br/>Mean<br/>Vector)
E --> G(Estimate<br/>Sample<br/>Covariance<br/>Matrix)
F --> H(Estimate<br/>Sample<br/>Correlation<br/>Matrix)
G --> I(Eigenvalues<br/>Eigenvectors)
H --> I

```

## Assumptions

- Variables should be continuous, interval or ratio level of measurement 
- Ordinal variables, such as likert-scale, are also often used
- Variables should all be the same scale/units
- Variables are linearly related
- Outliers & missing or impossible values should be removed


## Sample data 

### Sample Data

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  overflow:hidden;padding:10px 8px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  font-weight:normal;overflow:hidden;padding:10px 8px;word-break:normal;}
.tg .tg-vi6l{background-color:#ffffff;border-color:#8b814c;color:#8b814c;text-align:center;vertical-align:top}
.tg .tg-pi0b{background-color:#eaead6;border-color:#8b814c;color:#8b814c;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 791px">
<colgroup>
<col style="width: 126px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
</colgroup>
<thead>
  <tr>
    <th class="tg-pi0b"></th>
    <th class="tg-pi0b">$X_1$</th>
    <th class="tg-pi0b">$X_2$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_j$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_p$</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-pi0b">$x_1$</td>
    <td class="tg-vi6l">$x_{11}$</td>
    <td class="tg-vi6l">$x_{12}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_2$</td>
    <td class="tg-vi6l">$x_{21}$</td>
    <td class="tg-vi6l">$x_{22}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_i$</td>
    <td class="tg-vi6l">$x_{i1}$</td>
    <td class="tg-vi6l">$x_{i2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ij}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ip}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_n$</td>
    <td class="tg-vi6l">$x_{n1}$</td>
    <td class="tg-vi6l">$x_{n2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{nj}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{np}$</td>
  </tr>
</tbody>
</table>


## Equations

**Standardized Data:** $${z_{ij}}\ =\ \frac{{x_{ij}} - {\overline{x_j}}} {\sqrt{\hat{\sigma_{jj}}}}, \ \ \ \ \ where\ \ \ \ i\ =\ \{1,2,...,n\}, \ \ \ \ j\ =\ \{1,2,...,p\}$$ {#eq-stdzData}

**Random Vector:** $${x_i} = {({x_{i1}}, ... , {x_{ip}})}^T,\ \ \ \ \ \ \ \ \ where\ \ i\ =\ \{1,2,...,n\}$$ {#eq-randomVector}

**Sample Mean Vector:** $$\hat{\mu}\ \ =\ \ \overline{x}\ \ =\ \ \frac{1}{n} \sum_{i=1}^{n} x_i$$ {#eq-meanVector}


## Equations continued ...

**Sample Covariance Matrix:** $$\hat{\sum}\ \ =\ \ S\ \ =\ \ {(\hat{\sigma}_{ij})}_{p{\sf x}p}\ \ =\ \ \frac{1}{n-1} \sum_{i=1}^{n} {(x_i - \overline{x})(x_i - \overline{x})}^T$$ {#eq-covMatrix}

::: {.callout-note collapse="true"}
[The **sample mean vector** represents the center of the random vector
($x_i$)]{style="color: #1A79E1; font-size:12px;"}

[The **sample covariance matrix** represents the variations (diagonal elements) & correlations
(off-diagonal elements) of the random vector ($x_i$)]{style="color: #1A79E1; font-size:12px;"}
:::


## Equations continued ...

**Eigenvectors:** $${\hat{a}_{k}}$$ {#eq-eigenvec}

**Eigenvalues:** $${\hat{y}_{ik}}\ =\ {\hat{a}^T_{k}}(x_i\ -\ \overline{x}),\ \ \ \ \ \ \ i\ =\ \{1,2,...,n\},\ \ \ \ \ \ \ k\ =\ \{1,2,...,p\}$$ {#eq-eigenval}

<br>




## End













