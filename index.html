<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.1.189">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Brandy, Lee, Tavish">
<meta name="dcterms.date" content="2022-11-07">

<title>PCA (principal component analysis)</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1.6em;
  vertical-align: middle;
}
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { color: #008000; } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { color: #008000; font-weight: bold; } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>


<script src="index_files/libs/clipboard/clipboard.min.js"></script>
<script src="index_files/libs/quarto-html/quarto.js"></script>
<script src="index_files/libs/quarto-html/popper.min.js"></script>
<script src="index_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="index_files/libs/quarto-html/anchor.min.js"></script>
<link href="index_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="index_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="index_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="index_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="index_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">


</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">PCA (principal component analysis)</h1>
</div>



<div class="quarto-title-meta">

    <div>
    <div class="quarto-title-meta-heading">Author</div>
    <div class="quarto-title-meta-contents">
             <p>Brandy, Lee, Tavish </p>
          </div>
  </div>
    
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">November 7, 2022</p>
    </div>
  </div>
    
  </div>
  

</header>

<style>
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 .date {
    color: #191970;
  }
 body {
    background-color: #FAFAF5;
  }
</style>
<section id="introduction" class="level2">
<h2 class="anchored" data-anchor-id="introduction"><span style="color: #191970;">Introduction</span></h2>
<p><span style="color: #8B814C;"><strong>1. A comparative dimensionality reduction study in telecom customer segmentation using deep learning and PCA</strong></span></p>
<p><span style="color: #8B814C;">This article compares PCA with a deep learning autoencoder on telecom customer data. Goal is to segment customers based on 220 features to optimize customer satisfaction/loyalty. Raw data must be cleaned &amp; standardized (z-score normalization) before applying PCA. A scree plot was used to visualize the number of features to keep, which was 3 that explain ~72% of data. Using the absolute values of the eigenvectors they can decide wich features contribute the most to the first 3 PC’s. Overall PCA saved ~90% of total variance with just 20 features, reducing original dataset by 200. <span class="citation" data-cites="Alkha">(<a href="#ref-Alkha" role="doc-biblioref">Alkhayrat, Aljnidi, and Aljoumaa 2020</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>2. Visualizing Psychological Networks: A Tutorial in R</strong></span></p>
<p><span style="color: #8B814C;">Article aims to guide researchers on choosing the best/most interpretable visualizations of psychological networks. These networks include mental disorder symptoms (specifically OCD and depression) and the connections/correlations between them. When plotting, the symptoms are referred to as nodes and their connections as edges. The article goes on to compare 4 different plotting approaches; force-directed algorithms, multidimensional scaling, PCA, and eigenmodels. 6 different benefits were used as a checklist for all 4 approches. Benefits include: node placement is meaningful, useful for comparing replications, distances between nodes is interpretable, X/Y placement of nodes is interpretable, can be based on any network, central nodes in the center. PCA checks 3 of the 6 benefits, node placement is meaningful, useful for comparing replications, and X/Y placement of nodes is interpretable (this being the primary benefit since nodes/symptoms can be compared right to left, X, or top from bottom Y). 1 major disadvantage of PCA is that it relies on the correlation matrix and nodes/symptoms can be difficult to see when they score similarly on both PC’s. <span class="citation" data-cites="Jones">(<a href="#ref-Jones" role="doc-biblioref">Jones, Mair, and McNally 2018</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>3. SVM and PCA Based Learning Feature Classification Approaches for E-Learning System</strong></span></p>
<p><span style="color: #8B814C;">This article aims to classify student learning attributes using PCA in order to develop a simple methodology to optimize a students dynamic learning sequence based on their individual skill, needs and preferences, and also to maximize their learning outcome in computer programming courses (java, C). The study uses 8 different learning attributes; anxiety, personality, learning style, cognitive style, grades from prev. semester, motivation, study level, and student prior knowledge. 100 students taking C programming course were used in the present study. Each student filled out a questionairre to gather information on the 8 learning attributes. They each took a 20 question midterm and also a final that scored their capabilities in 3 areas; syntax, logical, and application (each score was divided into low, med, and high categories). For this study, 3 PC’s were kept &amp; used for classification purposes and explain ~77% of the variance. From figure 5 it looks like the first PC is comprised of 3 main learning attributes prior knowledge, learning style, and personality. Second PC groups motivation, cognitive style, and grades from prev. semester. Third PC groups anxiety with study level. The article then goes on to use the 3 PC’s to fit 4 classification models, a neural network, quadratic SVM, gaussian SVM, and linear SVM. Linear SVM provided the highest accuracy, sensitivity, and specificity, outperforming the other kernal classifiers. <span class="citation" data-cites="Khamp">(<a href="#ref-Khamp" role="doc-biblioref">Khamparia and Pandey 2018</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>4. Applied Multivariate Statistical Analysis and Related Topics with R</strong></span></p>
<p><span style="color: #8B814C;">This chapter goes over PCA basic’s and how and why it is useful. The overall goal of PCA is to minimize the number of predictive variables while maintaining most of the variation (70%~80% as a guideline, the book goes on to cover a few more rules of thumb for this). PCA is a good method for finding outliers since the principal components are linear combinations of the original data - plotting with lower dimensions makes them easier to spot. PCA is also useful when some of the predictor variables in a regression model are highly correlated, which if not addressed can lead to poor parameter estimates. The first PC explains the most variability in the data &amp; each succeeding PC explains the most possible remaining variability. PCA analysis should be preformed on scaled/unit data, magnitudes must be comparable (use the correlation matrix instead of covariance OR standardize the data). <span class="citation" data-cites="lang2021applied">(<a href="#ref-lang2021applied" role="doc-biblioref">Lang and Qiu 2021</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>5. The fixed effects PCA model in a common principal component environment</strong></span></p>
<p><span style="color: #8B814C;">The article compares a fixed effects PCA model to the 2 most common approaches, descriptive algebraic and probabilistic. All three produce the same results using spectral decomposition of the sample covariance matrix but, the interpretations will differ depending on the assumptions. Graphing the low dimensional PC’s (usually the first 2) is a common way to identify hidden patterns in the data such as outliers or clusters. The fixed effects model only makes assumptions about the dimensionality of the data, and incorporates knowledge about noise in the data to improve estimates. The results of the paper were that the fixed effects model incorporating CPCA (common PCA) out preformed all others. <span class="citation" data-cites="duras2022fixed">(<a href="#ref-duras2022fixed" role="doc-biblioref">Duras 2022</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>6. Microaneurysm Detection Using Principal Component Analysis and Machine Learning Methods</strong></span></p>
<p><span style="color: #8B814C;">The Following paper follows the use of Principal Component Analysis and Machine Learning Methods with recognizing Microanerysm. Here Diabetic Retinopathy (DR) is a progressive disease with almost no early symptoms of vision impairment,which is the leading cause of blindness prior to the age. As a result The first detectable sign of DR is the presence of microaneurysms (MAs), which result from leakage of tiny blood vessels in the retina and manifest themselves as small red circular spots on the surface of retinas. Early detection of MAs is critical for diagnosis and treatment of DR, which has led to a great deal of research towards automatic detection of MAs. The paper relays that Many existing MA detection methods rely on hand-crafted features, which are often based on low-level information. Lowlevel information is easily susceptible to signal drift artifacts and thus prevent reliable generalization among different research sites. A recent method [1] leveraged the use of deep learning for MA detection using a Stacked Sparse Autoencoder (SSAE). Deep learning approaches often learn high-level and robust attributes directly from the raw signal input, and have been successfully applied to various classification and recognition tasks [8]–[10]. In [1], small image patches were generated from the original fundus images and used by the SSAE to learn high-level features from pixel intensities. These patches were then classified as either MA. The goal is to determine whether traditional machine learning methods can achieve similar or better performance on the same fundus image dataset by exploring the full context of the image information, especially when the size of the dataset may be too limited for reliably training deep learning The Dataset analyzed DIARETDB1 [11] images that were acquired from 89 patients, in which each image was manually annotated by multiple experts at Kuopio hospital. 84 of the 89 patients contained at least mild non-proliferative signs of DR, while the remaining five patients were healthy with no signs of DR. Classification of MA vs non-MA patches was performed using three sets of features. The first feature set consisted of raw pixel intensities, rasterized from the image patch. principal component analysis (PCA). A common feature dimensionality reduction method. PCA projects data onto a new space in which consecutive dimensions contain less and less of the variance of the original data space and compresses the most important information onto a subspace with lower dimensionality than the original space. The machine learning methods employed in this work include RF, NN, and SVM. the methods outperformed previous work that conducted deep learning using the same database, measured by both AUC and F-measure. Further validation of our method on a different dataset ROC showed similar results as we found on DIARETDB1. Our results yield a promising step towards automated early detection of microaneurysms and diabetic retinopathy. Three interesting results were observed in our findings. First, after application of PCA to the original 625-dimensional dataspace, SVMs achieved near maximum performance using only 30% of data. With the incorporation of more principal components, the performance of the SVM plateaued, while the performance of the RF and NN both decreased. Second, by reducing dimensionality using RF feature importance, they were able to achieve high AUC with fewer dimensions and maintain this high AUC as more dimensions were included. Third, when we trained the random forest on DIARETDB1 and used ROC as the testing data, the performance was similar as that achieved by the random forest trained and tested on the ROC dataset using cross-validation. <span class="citation" data-cites="Cao2018">(<a href="#ref-Cao2018" role="doc-biblioref">Cao et al. 2018</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>7. PCA-based polling strategy in machine learning framework for coronary artery disease risk assessment in intravascular ultrasound: A link between carotid and coronary grayscale plaque morphology</strong></span></p>
<p><span style="color: #8B814C;">The following paper reflects on a novel strategy for risk stratification based on plaque morphology embedded with principal component analysis (PCA) for plaque feature dimensionality reduction and dominant feature selection technique. The paper leads the major cause of morbidity in the world is due to cardiovascular disease (CVD). These diseases occur due to atherosclerosis a progressive and slow process of narrowing the artery, interrupting the flow of blood from the heart or to the brain. Today The current-state-of-art methods for screening the severity of this disease is: computed tomography (CT), ultrasound(US), and magnetic resonance imaging (MRI). However Due to radiation ,CT may compromise the patients’ safety, but it is often used because it computes a calcium score in the coronary artery. It cover two hypothesis : First : We thus hypothesize that different components offer different risk factors and when combined as a whole using the grayscale wall region image can be used for tissue characterization. We can thus lever-age to study the morphological characteristics of these lesions and adapt a machine learning paradigm to predict the risk of severity of CAD leading to myocardial infarction. Here the paper will explores the novel concept of morphological characteristics utilizing the coronary vessel wall region that possesses these plaque components. Second : They hypothesize that cIMT can be adapted for developing a link between the coronary plaque burden leading to coronary artery disease or carotid artery disease leading to stroke. This is due to the correlation between automated cIMT that includes bulb plaque and SYNTAX score. Based on the experiment regarding the role of the two hypotheses, they present the two experimental protocols based on that foundation. The two major hypotheses were: (i) risk associated with the components of the coronary artery vessel wall and (ii) ability of carotid IMT to characterize the cardiovascular risk. There protocol design demonstrates the machine learning paradigm for risk assessment using the combination of (a) grayscale PCA-based dominant features of the coronary artery wall and (b) the gold standard subclinical risk biomarker – the carotid IMT during the learning-phase that generates the offline coefficients. The testing-phase utilizes the PCA-based dominant feature extraction which is then transformed by the offline coefficients. Based on Results the best feature combination using PCA-based polling. The section then details which kernel function is best suitable using the PCA-based optimization for a fixed data size N. It is then concluded that presented a coronary artery risk assessment system by taking two key hypothesis: (i) there is a coronary artery disease risk associated with vessel wall region consisting of different plaque components such as: fibrous, fibrolipidic, calcified and calcified-necrotic; (ii) coronary risk label is derived using the carotid intima-media thickness biomarker that was used as a gold standard for design and development of machine learning system for coronary artery disease risk assessment and stratification. The computer-aided diagnosis system based on machine learning utilizing PCA-based feature selection criteria is able to classify the high risk and low risk coronary artery disease patients with high accuracy reaching close to 98.50%. We demonstrated that PCA-based feature selection using polling method is highly suitable for dominant features selection. The system showed a high reliability of 97.32% while meeting the stability criteria of 5%. The coronary artery disease risk assessment is automated given the vessel wall region of the coronary artery. The results are promising leading to the prototype design for a clinical setup. <span class="citation" data-cites="Arak2016">(<a href="#ref-Arak2016" role="doc-biblioref">Araki et al. 2016</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>8. Facial recognition with PCA and machine learning methods</strong></span></p>
<p><span style="color: #8B814C;">From the this Paper the authors have identified that the best way to compare and evaluate the Facial Recognition results with speed and accuracy was with PCA (Principal Component Analysis) and as well as Support Vector Machine (SVM). PCA is a to find the principal components of a given set of images and represent each face image as a smaller size in lower dimensional face space using the eigenvectors that correspond to higher eigenvalues. Here the author see’s the PCA provides an expression of an input image in terms of a set of basis images that appear as “EigenFaces”. The eigenfaces can then be used to identify the individual face from the set of ORL Data Base of Faces. In the SVM a Given a set of training examples were created and each of them is marked as belonging to one class. An SVM training algorithm was build to a model to assign a new example to one class. The SVM model, is a representation of the examples as points in space and maps these examples to classes, which are divided by a clear gap as wide as possible. There New examples can be mapped into the same space and predicted to belong to the class based on which side of the gap they fall. Other Mentions were that KNM K Nearest Neighbor another well-known method of classification was used and that uses each class is a cluster and the data points belong to a cluster. Where the locations of clusters need to be determined (cluster center), as well as which data points belong to which cluster. It finally concludes that based on the three different methods of facial recognition it showed that SVM/PCA is the best choice for data with little sensitivity to running speed and a strong desire to get higher recognition accuracy.<span class="citation" data-cites="Chen2017">(<a href="#ref-Chen2017" role="doc-biblioref">Chen and Jenkins 2017</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>9. A Comparison of Linear and Non-Linear Machine Learning Techniques (PCA and SOM) for Characterizing Urban Nutrient Runoff</strong></span></p>
<p><span style="color: #8B814C;">The following Study was based on an analysis of the Linear Machine learning ( In this Paper we look at , the principal component analysis (PCA) for the linear technique and the self-organizing map (SOM) for the nonlinear technique) and supported by Spearman’s rank correlation coefficient (ρ). The Dataset for the models was from a monitoring campaign was carried out in SB to collect precipitation, flow rate, and water quality records. Summaries of the observed rainfall-runoff (antecedent dry period (ADP), total rainfall, event duration, maximum rainfall intensity, runoff volume, and runoff peak) and water quality (minimum, maximum, and event mean concentration (EMC)) data are presented in Tables 1 and 2. Here the main objective of the ML of PCA and SOM is reducing the dimensionality of a dataset that contains a large number of interrelated variables while preserving most of the dataset variance that was collected. Each Data Algorithm was used in Python3.8 when simulated and The first aspect that was compared between PCA and SOM was the ability to correlate rainfall characteristics to water quality variables, in other words, to correctly represent build-up and wash-off processes To conclude this experiment it was understood that the primary purpose of this work was the comparison of linear and non-linear ML techniques, PCA and SOM, respectively, to characterize urban nutrient runoff. In particular, this comparison was based on three main aspects: (i) the ability to represent the correlation among the variables chosen to represent the system and, therefore, depict the build-up and wash-off processes (cause–effect process) (feature correlation); (ii) the ability to group the dataset based on the two variables that symbolize build-up and wash-off processes (data point grouping); (iii) the ability to identify and quantify the importance of each variable (feature importance). <span class="citation" data-cites="Gorg2021">(<a href="#ref-Gorg2021" role="doc-biblioref">Gorgoglione et al. 2021</a>)</span></span></p>
<p><span style="color: #8B814C;"><strong>10. Machine Learning based Multiscale Reduced Kernel PCA for Nonlinear Process Monitoring</strong></span></p>
<p><span style="color: #8B814C;">This paper surround the importance of monitoring Faulty Detections. Fault detection (FD) is fundamental for monitoring several chemical processes. Thus, this paper introduces a novel structure multiscale reduced kernel principal component analysis (MS-RKPCA). Here the paper aims to address the problem of great computation time and significant storage memory space by using a data reduction structure based on the Euclidean distance metric. The paper talks about Process monitoring of chemical systems is important and essential to observe the best functioning and to ameliorate the product quality of various industrial processes. Here it shows Many data-driven techniques have been proposed and they have been statistically assessed for linearly separable data. The linearly data driven approaches including principal component analysis(PCA), and partial least squares (PLS) . These techniques have been widely applied for dimensionality reduction and they are effective for modeling and monitoring chemical processes . PCA is extensively applied for monitoring industrial process. It has proven successful results for fault detection of the linear process But now However, popular complex chemical systems exhibit a great nonlinear correlation between their variables. Then, common chemical systems demand nonlinear techniques in order to execute tasks that implicate successfully modeling and monitoring methods Recently, a nonlinear PCA (KPCA) method has been proposed KPCA technique can efficiently operate the intricate spatial structure of high-dimensional feature spaces using integral operators and the notion of kernel tricks . KPCA method purposes to project the input data onto the feature space by nonlinear kernel functions and then to perform PCA in that feature space. However most complex industrial process data are extremely correlated which leads to a problem when analyzing large process data . Therefore, the first contribution of this paper is to propose an alternative method called reduced KPCA (RKPCA) for dealing with the issue of intended storage space, and elevated computation time. The improved RKPCA method consists of using the Euclidean distance to keep an observation in case of redundancy between observations. It is noted Thus, RKPCA accelerates the evaluation of the test sample and also saves the memory of keeping the trained sample. The obtained reduced data contains the most relevant information about the dataset. To be effective, any FD technique must rely on the goodness of the process data. To conclude , a technique that merges a size reduction framework and wavelet analysis is proposed to monitor nonlinear processes and enhance the fault detection ability of conventional KPCA. The key view of the developed technique is to apply the multiscale representation on the reduced dataset using the Euclidean distance metric in order to improve the modeling abilities and provide good effectiveness over the KPCA model. The Tennessee Eastman Process (TEP) data is used to validate the effectiveness of the develop method. The results demonstrate the effectiveness of the proposed multiscale reduced KPCA (MS-RKPCA) approach over both RKPCA and KPCA models to increase the computation times and to ensure the quality of the detection (MDR and FAR).<span class="citation" data-cites="Dhibi2020">(<a href="#ref-Dhibi2020" role="doc-biblioref">Dhibi et al. 2020</a>)</span></span></p>
</section>
<section id="methods" class="level2">
<h2 class="anchored" data-anchor-id="methods"><span style="color: #191970;">Methods</span></h2>
</section>
<section id="analysis-and-results" class="level2">
<h2 class="anchored" data-anchor-id="analysis-and-results"><span style="color: #191970;">Analysis and Results</span></h2>
</section>
<section id="data-and-vizualisation" class="level2">
<h2 class="anchored" data-anchor-id="data-and-vizualisation"><span style="color: #191970;">Data and Vizualisation</span></h2>
<p><span style="color: #8B814C;">Bacteria dataset from MASS package (220 rows by 6 columns)</span></p>
<p><span style="color: #8B814C;">Dr A. Leach tested the effects of a drug on 50 children with a history of otitis media in the Northern Territory of Australia. The children were randomized to the drug or the a placebo, and also to receive active encouragement to comply with taking the drug. The presence of H. influenzae was checked at weeks 0, 2, 4, 6 and 11: 30 of the checks were missing and are not included in this data frame.</span></p>
<p><span style="color: #8B814C;"><strong>Variables</strong></span></p>
<p><span style="color: #8B814C;"><strong>y</strong> - presence or absence: a factor with levels n and y</span></p>
<p><span style="color: #8B814C;"><strong>ap</strong> - active/placebo: a factor with levels a and p</span></p>
<p><span style="color: #8B814C;"><strong>hilo</strong> - hi/low compliance: a factor with levels hi amd lo</span></p>
<p><span style="color: #8B814C;"><strong>week</strong> - numeric: week of test</span></p>
<p><span style="color: #8B814C;"><strong>ID</strong> - subject ID: a factor</span></p>
<p><span style="color: #8B814C;"><strong>trt</strong> - a factor with levels placebo, drug and drug+, a re-coding of ap and hilo</span></p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="co"># loading packages </span></span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(tidyverse)</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(knitr)</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggthemes)</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(ggrepel)</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(dslabs)</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="fu">library</span>(MASS)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="co"># Load Data</span></span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a><span class="fu">kable</span>(<span class="fu">head</span>(bacteria))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output-display">
<table class="table table-sm table-striped">
<thead>
<tr class="header">
<th style="text-align: left;">y</th>
<th style="text-align: left;">ap</th>
<th style="text-align: left;">hilo</th>
<th style="text-align: right;">week</th>
<th style="text-align: left;">ID</th>
<th style="text-align: left;">trt</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">y</td>
<td style="text-align: left;">p</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">X01</td>
<td style="text-align: left;">placebo</td>
</tr>
<tr class="even">
<td style="text-align: left;">y</td>
<td style="text-align: left;">p</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">X01</td>
<td style="text-align: left;">placebo</td>
</tr>
<tr class="odd">
<td style="text-align: left;">y</td>
<td style="text-align: left;">p</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">4</td>
<td style="text-align: left;">X01</td>
<td style="text-align: left;">placebo</td>
</tr>
<tr class="even">
<td style="text-align: left;">y</td>
<td style="text-align: left;">p</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">11</td>
<td style="text-align: left;">X01</td>
<td style="text-align: left;">placebo</td>
</tr>
<tr class="odd">
<td style="text-align: left;">y</td>
<td style="text-align: left;">a</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">0</td>
<td style="text-align: left;">X02</td>
<td style="text-align: left;">drug+</td>
</tr>
<tr class="even">
<td style="text-align: left;">y</td>
<td style="text-align: left;">a</td>
<td style="text-align: left;">hi</td>
<td style="text-align: right;">2</td>
<td style="text-align: left;">X02</td>
<td style="text-align: left;">drug+</td>
</tr>
</tbody>
</table>
</div>
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode r code-with-copy"><code class="sourceCode r"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="co">#ggplot1 = murders %&gt;% ggplot(mapping = aes(x=population/10^6, y=total)) </span></span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>  <span class="co">#ggplot1 + geom_point(aes(col=region), size = 4) +</span></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>  <span class="co">#geom_text_repel(aes(label=abb)) +</span></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>  <span class="co">#scale_x_log10() +</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>  <span class="co">#scale_y_log10() +</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>  <span class="co">#geom_smooth(formula = "y~x", method=lm,se = F)+</span></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a>  <span class="co">#xlab("Populations in millions (log10 scale)") + </span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>  <span class="co">#ylab("Total number of murders (log10 scale)") +</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>  <span class="co">#ggtitle("US Gun Murders in 2010") +</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>  <span class="co">#scale_color_discrete(name = "Region")+</span></span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a>  <span class="co">#    theme_wsj()</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
</section>
<section id="statistical-modeling" class="level2">
<h2 class="anchored" data-anchor-id="statistical-modeling"><span style="color: #191970;">Statistical Modeling</span></h2>
</section>
<section id="conlusion" class="level2">
<h2 class="anchored" data-anchor-id="conlusion"><span style="color: #191970;">Conlusion</span></h2>
</section>
<section id="references" class="level2 unnumbered">


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="doc-bibliography">
<div id="ref-Alkha" class="csl-entry" role="doc-biblioentry">
Alkhayrat, Maha, Mohamad Aljnidi, and Kadan Aljoumaa. 2020. <span>“A Comparative Dimensionality Reduction Study in Telecom Customer Segmentation Using Deep Learning and PCA.”</span> <em>Journal of Big Data</em> 7 (February): 9. <a href="https://doi.org/10.1186/s40537-020-0286-0">https://doi.org/10.1186/s40537-020-0286-0</a>.
</div>
<div id="ref-Arak2016" class="csl-entry" role="doc-biblioentry">
Araki, Tadashi, Nobutaka Ikeda, Devarshi Shukla, Pankaj K. Jain, Narendra D. Londhe, Vimal K. Shrivastava, Sumit K. Banchhor, et al. 2016. <span>“PCA-Based Polling Strategy in Machine Learning Framework for Coronary Artery Disease Risk Assessment in Intravascular Ultrasound.”</span> <em>Comput. Methods Prog. Biomed.</em> 128 (C): 137–58.
</div>
<div id="ref-Cao2018" class="csl-entry" role="doc-biblioentry">
Cao, Wen, Nicholas Czarnek, Juan Shan, and Lin Li. 2018. <span>“Microaneurysm Detection Using Principal Component Analysis and Machine Learning Methods.”</span> <em>IEEE Transactions on NanoBioscience</em> 17 (3): 191–98. <a href="https://doi.org/10.1109/TNB.2018.2840084">https://doi.org/10.1109/TNB.2018.2840084</a>.
</div>
<div id="ref-Chen2017" class="csl-entry" role="doc-biblioentry">
Chen, Jiachen, and W. Kenneth Jenkins. 2017. <span>“Facial Recognition with PCA and Machine Learning Methods.”</span> In <em>2017 IEEE 60th International Midwest Symposium on Circuits and Systems (MWSCAS)</em>, 973–76. <a href="https://doi.org/10.1109/MWSCAS.2017.8053088">https://doi.org/10.1109/MWSCAS.2017.8053088</a>.
</div>
<div id="ref-Dhibi2020" class="csl-entry" role="doc-biblioentry">
Dhibi, Khaled, Radhia Fezai, Kais Bouzrara, Majdi Mansouri, Abdelmalek Kouadri, and Mohamed-Faouzi Harkat. 2020. <span>“Machine Learning Based Multiscale Reduced Kernel PCA for Nonlinear Process Monitoring.”</span> In <em>2020 17th International Multi-Conference on Systems, Signals &amp;Amp; Devices (SSD)</em>, 302–7. <a href="https://doi.org/10.1109/SSD49366.2020.9364160">https://doi.org/10.1109/SSD49366.2020.9364160</a>.
</div>
<div id="ref-duras2022fixed" class="csl-entry" role="doc-biblioentry">
Duras, Toni. 2022. <span>“The Fixed Effects PCA Model in a Common Principal Component Environment.”</span> <em>Communications in Statistics-Theory and Methods</em> 51 (6): 1653–73.
</div>
<div id="ref-Gorg2021" class="csl-entry" role="doc-biblioentry">
Gorgoglione, Angela, Alberto Castro, V. Iacobellis, and Gioia Andrea. 2021. <span>“A Comparison of Linear and Non-Linear Machine Learning Techniques (PCA and SOM) for Characterizing Urban Nutrient Runoff.”</span> <em>Sustainability</em> 13 (February): 2054. <a href="https://doi.org/10.3390/su13042054">https://doi.org/10.3390/su13042054</a>.
</div>
<div id="ref-Jones" class="csl-entry" role="doc-biblioentry">
Jones, Payton, Patrick Mair, and Richard McNally. 2018. <span>“Visualizing Psychological Networks: A Tutorial in r.”</span> <em>Frontiers in Psychology</em> 9 (September). <a href="https://doi.org/10.3389/fpsyg.2018.01742">https://doi.org/10.3389/fpsyg.2018.01742</a>.
</div>
<div id="ref-Khamp" class="csl-entry" role="doc-biblioentry">
Khamparia, Aditya, and Babita Pandey. 2018. <span>“SVM and PCA Based Learning Feature Classification Approaches for e-Learning System.”</span> <em>International Journal of Web-Based Learning and Teaching Technologies</em> 13 (April): 32–45. <a href="https://doi.org/10.4018/IJWLTT.2018040103">https://doi.org/10.4018/IJWLTT.2018040103</a>.
</div>
<div id="ref-lang2021applied" class="csl-entry" role="doc-biblioentry">
Lang, WU, and Jin Qiu. 2021. <em>Applied Multivariate Statistical Analysis and Related Topics with r</em>. edp sciences.
</div>
</div></section></div></main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    target: function(trigger) {
      return trigger.previousElementSibling;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    setTimeout(function() {
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const cites = ref.parentNode.getAttribute('data-cites').split(' ');
    tippyHover(ref, function() {
      var popup = window.document.createElement('div');
      cites.forEach(function(cite) {
        var citeDiv = window.document.createElement('div');
        citeDiv.classList.add('hanging-indent');
        citeDiv.classList.add('csl-entry');
        var biblioDiv = window.document.getElementById('ref-' + cite);
        if (biblioDiv) {
          citeDiv.innerHTML = biblioDiv.innerHTML;
        }
        popup.appendChild(citeDiv);
      });
      return popup.innerHTML;
    });
  }
});
</script>
</div> <!-- /content -->



</body></html>