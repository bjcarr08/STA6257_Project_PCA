---
title: "PCA (principal component analysis)"
author: "Brandy, Lee, Tavish"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
    highlight: pygments
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown: 
    wrap: 72
---

```{=html}
<style>
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 .date {
    color: #191970;
  }
 body {
    background-color: #FAFAF5;
  }
</style>
```
## [Introduction]{style="color: #191970;"}

[Principal Component Analysis (PCA) is the name for a process of reducing the dimension of a large data set (when the observations one wishes to compare is described by many variables) into a smaller simplified version of the original data set that retains the most salient features of the data. PCA is a broadly used statistical method whose use stretches across many scientific disciplines, and in turn many different adaptations of PCA have been developed based on the variation in goals and data types associated with each respective discipline. PCA was developed first by Pearson (1901) and Hotelling (1933) [@Camargo]. This technique transforms some number of possibly correlated variables into a smaller number of variables, the variables in this smaller matrix are referred to as the Principal Components (PC). This is achieved using a vector space transform. By mathematical projection we can interpret the original data set with just a few variables, namely the principal components achieved through calculation. The purpose of reducing dimension size in large data sets is to make it easier to spot trends, patterns, and outliers in data where that information would have previously been hidden by the size of data set of interest (Richardson). The information being preserved in the process of reduction is the variability in the data (i.e. the statistical information present in the data). In order to preserve as much variability as possible we should "…find new variables that are linear functions of those in the original data set, that successively maximize variance and that are uncorrelated with each other" [@Jolliffe]. PCA assumes no distribution in data in a descriptive context which is one of it's key features that makes it so widely adaptive as an exploratory method across disciplines and data types [@Jolliffe]. To lists all of PCA's applications would be tedious and excessive, but some examples where PCA has been used is in facial recognition, image analysis, analysis of web data, and cyber security analysis. Essentially anywhere that large data sets are found PCA can be used to aid in discovering trends amongst the variables of that data. One key field PCA has seen use is in the analysis of cyber security network data. PCA could see many applications in web data analysis as the amount of data for this purpose is always increasing and becoming more relevant.]{style="color: #8B814C;"}

[Taking a look into a real world example, say we have a dataset consisting of 1,000 students with exam scores from 7 different courses: Advanced Statistics, Probability Theory, Intro to Dance, World Religions, and Religion in America. We could group Advanced Statistics and Probability Theory into a new variable called Stats, group World Religions and Religion in America into a new variable Religion, and keep Intro to Dance by itself. We have Reduced the data set from 7 variables to 3 without much loss in variation. This is the main concept behind PCA except variables are not manually regrouped but instead the new variables (principal components) are derived from certain linear combinations of the original variables [@lang2021applied].]{style="color: #8B814C;"}

[PCA can also be useful when studying mental disorders, data consisting of symptoms and the connections between them. When many symptoms are being observed, it can be difficult to visually represent the connections between them, both the strength of the connections and the proximity to each other. Plotting using the first 2 principal components allows for the placement on the x or y axis to become interpretable, that is, observations far left differ in some dimension (the first principal component) compared to ones far right. The same can be said in the y direction [@Jones].]{style="color: #8B814C;"}


**Individual Lit Reviews (will be removed when intro is completed)**

[**1. A comparative dimensionality reduction study in telecom customer
segmentation using deep learning and PCA**]{style="color: #8B814C;"}

[This article compares PCA with a deep learning autoencoder on telecom
customer data. Goal is to segment customers based on 220 features to
optimize customer satisfaction/loyalty. Raw data must be cleaned &
standardized (z-score normalization) before applying PCA. A scree plot
was used to visualize the number of features to keep, which was 3 that
explain \~72% of data. Using the absolute values of the eigenvectors
they can decide wich features contribute the most to the first 3 PC's.
Overall PCA saved \~90% of total variance with just 20 features,
reducing original dataset by 200. [@Alkha]]{style="color: #8B814C;"}

[**2. Visualizing Psychological Networks: A Tutorial in
R**]{style="color: #8B814C;"}

[Article aims to guide researchers on choosing the best/most
interpretable visualizations of psychological networks. These networks
include mental disorder symptoms (specifically OCD and depression) and
the connections/correlations between them. When plotting, the symptoms
are referred to as nodes and their connections as edges. The article
goes on to compare 4 different plotting approaches; force-directed
algorithms, multidimensional scaling, PCA, and eigenmodels. 6 different
benefits were used as a checklist for all 4 approches. Benefits include:
node placement is meaningful, useful for comparing replications,
distances between nodes is interpretable, X/Y placement of nodes is
interpretable, can be based on any network, central nodes in the center.
PCA checks 3 of the 6 benefits, node placement is meaningful, useful for
comparing replications, and X/Y placement of nodes is interpretable
(this being the primary benefit since nodes/symptoms can be compared
right to left, X, or top from bottom Y). 1 major disadvantage of PCA is
that it relies on the correlation matrix and nodes/symptoms can be
difficult to see when they score similarly on both PC's.
[@Jones]]{style="color: #8B814C;"}

[**3. SVM and PCA Based Learning Feature Classification Approaches for
E-Learning System**]{style="color: #8B814C;"}

[This article aims to classify student learning attributes using PCA in
order to develop a simple methodology to optimize a students dynamic
learning sequence based on their individual skill, needs and
preferences, and also to maximize their learning outcome in computer
programming courses (java, C). The study uses 8 different learning
attributes; anxiety, personality, learning style, cognitive style,
grades from prev. semester, motivation, study level, and student prior
knowledge. 100 students taking C programming course were used in the
present study. Each student filled out a questionairre to gather
information on the 8 learning attributes. They each took a 20 question
midterm and also a final that scored their capabilities in 3 areas;
syntax, logical, and application (each score was divided into low, med,
and high categories). For this study, 3 PC's were kept & used for
classification purposes and explain \~77% of the variance. From figure 5
it looks like the first PC is comprised of 3 main learning attributes
prior knowledge, learning style, and personality. Second PC groups
motivation, cognitive style, and grades from prev. semester. Third PC
groups anxiety with study level. The article then goes on to use the 3
PC's to fit 4 classification models, a neural network, quadratic SVM,
gaussian SVM, and linear SVM. Linear SVM provided the highest accuracy,
sensitivity, and specificity, outperforming the other kernal
classifiers. [@Khamp]]{style="color: #8B814C;"}

[**4. Applied Multivariate Statistical Analysis and Related Topics with
R**]{style="color: #8B814C;"}

[This chapter goes over PCA basic's and how and why it is useful. The
overall goal of PCA is to minimize the number of predictive variables
while maintaining most of the variation (70%\~80% as a guideline, the
book goes on to cover a few more rules of thumb for this). PCA is a good
method for finding outliers since the principal components are linear
combinations of the original data - plotting with lower dimensions makes
them easier to spot. PCA is also useful when some of the predictor
variables in a regression model are highly correlated, which if not
addressed can lead to poor parameter estimates. The first PC explains
the most variability in the data & each succeeding PC explains the most
possible remaining variability. PCA analysis should be preformed on
scaled/unit data, magnitudes must be comparable (use the correlation
matrix instead of covariance OR standardize the data).
[@lang2021applied]]{style="color: #8B814C;"}

[**5. The fixed effects PCA model in a common principal component
environment**]{style="color: #8B814C;"}

[The article compares a fixed effects PCA model to the 2 most common
approaches, descriptive algebraic and probabilistic. All three produce
the same results using spectral decomposition of the sample covariance
matrix but, the interpretations will differ depending on the
assumptions. Graphing the low dimensional PC's (usually the first 2) is
a common way to identify hidden patterns in the data such as outliers or
clusters. The fixed effects model only makes assumptions about the
dimensionality of the data, and incorporates knowledge about noise in
the data to improve estimates. The results of the paper were that the
fixed effects model incorporating CPCA (common PCA) out preformed all
others. [@duras2022fixed]]{style="color: #8B814C;"}

[**6. Microaneurysm Detection Using Principal Component Analysis and
Machine Learning Methods**]{style="color: #8B814C;"}

[The Following paper follows the use of Principal Component Analysis and
Machine Learning Methods with recognizing Microanerysm. Here Diabetic
Retinopathy (DR) is a progressive disease with almost no early symptoms
of vision impairment,which is the leading cause of blindness prior to
the age. As a result The first detectable sign of DR is the presence of
microaneurysms (MAs), which result from leakage of tiny blood vessels in
the retina and manifest themselves as small red circular spots on the
surface of retinas. Early detection of MAs is critical for diagnosis and
treatment of DR, which has led to a great deal of research towards
automatic detection of MAs. The paper relays that Many existing MA
detection methods rely on hand-crafted features, which are often based
on low-level information. Lowlevel information is easily susceptible to
signal drift artifacts and thus prevent reliable generalization among
different research sites. A recent method \[1\] leveraged the use of
deep learning for MA detection using a Stacked Sparse Autoencoder
(SSAE). Deep learning approaches often learn high-level and robust
attributes directly from the raw signal input, and have been
successfully applied to various classification and recognition tasks
\[8\]--\[10\]. In \[1\], small image patches were generated from the
original fundus images and used by the SSAE to learn high-level features
from pixel intensities. These patches were then classified as either MA.
The goal is to determine whether traditional machine learning methods
can achieve similar or better performance on the same fundus image
dataset by exploring the full context of the image information,
especially when the size of the dataset may be too limited for reliably
training deep learning The Dataset analyzed DIARETDB1 \[11\] images that
were acquired from 89 patients, in which each image was manually
annotated by multiple experts at Kuopio hospital. 84 of the 89 patients
contained at least mild non-proliferative signs of DR, while the
remaining five patients were healthy with no signs of DR. Classification
of MA vs non-MA patches was performed using three sets of features. The
first feature set consisted of raw pixel intensities, rasterized from
the image patch. principal component analysis (PCA). A common feature
dimensionality reduction method. PCA projects data onto a new space in
which consecutive dimensions contain less and less of the variance of
the original data space and compresses the most important information
onto a subspace with lower dimensionality than the original space. The
machine learning methods employed in this work include RF, NN, and SVM.
the methods outperformed previous work that conducted deep learning
using the same database, measured by both AUC and F-measure. Further
validation of our method on a different dataset ROC showed similar
results as we found on DIARETDB1. Our results yield a promising step
towards automated early detection of microaneurysms and diabetic
retinopathy. Three interesting results were observed in our findings.
First, after application of PCA to the original 625-dimensional
dataspace, SVMs achieved near maximum performance using only 30% of
data. With the incorporation of more principal components, the
performance of the SVM plateaued, while the performance of the RF and NN
both decreased. Second, by reducing dimensionality using RF feature
importance, they were able to achieve high AUC with fewer dimensions and
maintain this high AUC as more dimensions were included. Third, when we
trained the random forest on DIARETDB1 and used ROC as the testing data,
the performance was similar as that achieved by the random forest
trained and tested on the ROC dataset using cross-validation.
[@Cao2018]]{style="color: #8B814C;"}

[**7. PCA-based polling strategy in machine learning framework for
coronary artery disease risk assessment in intravascular ultrasound: A
link between carotid and coronary grayscale plaque
morphology**]{style="color: #8B814C;"}

[The following paper reflects on a novel strategy for risk
stratification based on plaque morphology embedded with principal
component analysis (PCA) for plaque feature dimensionality reduction and
dominant feature selection technique. The paper leads the major cause of
morbidity in the world is due to cardiovascular disease (CVD). These
diseases occur due to atherosclerosis a progressive and slow process of
narrowing the artery, interrupting the flow of blood from the heart or
to the brain. Today The current-state-of-art methods for screening the
severity of this disease is: computed tomography (CT), ultrasound(US),
and magnetic resonance imaging (MRI). However Due to radiation ,CT may
compromise the patients' safety, but it is often used because it
computes a calcium score in the coronary artery. It cover two hypothesis
: First : We thus hypothesize that different components offer different
risk factors and when combined as a whole using the grayscale wall
region image can be used for tissue characterization. We can thus
lever-age to study the morphological characteristics of these lesions
and adapt a machine learning paradigm to predict the risk of severity of
CAD leading to myocardial infarction. Here the paper will explores the
novel concept of morphological characteristics utilizing the coronary
vessel wall region that possesses these plaque components. Second : They
hypothesize that cIMT can be adapted for developing a link between the
coronary plaque burden leading to coronary artery disease or carotid
artery disease leading to stroke. This is due to the correlation between
automated cIMT that includes bulb plaque and SYNTAX score. Based on the
experiment regarding the role of the two hypotheses, they present the
two experimental protocols based on that foundation. The two major
hypotheses were: (i) risk associated with the components of the coronary
artery vessel wall and (ii) ability of carotid IMT to characterize the
cardiovascular risk. There protocol design demonstrates the machine
learning paradigm for risk assessment using the combination of (a)
grayscale PCA-based dominant features of the coronary artery wall and
(b) the gold standard subclinical risk biomarker -- the carotid IMT
during the learning-phase that generates the offline coefficients. The
testing-phase utilizes the PCA-based dominant feature extraction which
is then transformed by the offline coefficients. Based on Results the
best feature combination using PCA-based polling. The section then
details which kernel function is best suitable using the PCA-based
optimization for a fixed data size N. It is then concluded that
presented a coronary artery risk assessment system by taking two key
hypothesis: (i) there is a coronary artery disease risk associated with
vessel wall region consisting of different plaque components such as:
fibrous, fibrolipidic, calcified and calcified-necrotic; (ii) coronary
risk label is derived using the carotid intima-media thickness biomarker
that was used as a gold standard for design and development of machine
learning system for coronary artery disease risk assessment and
stratification. The computer-aided diagnosis system based on machine
learning utilizing PCA-based feature selection criteria is able to
classify the high risk and low risk coronary artery disease patients
with high accuracy reaching close to 98.50%. We demonstrated that
PCA-based feature selection using polling method is highly suitable for
dominant features selection. The system showed a high reliability of
97.32% while meeting the stability criteria of 5%. The coronary artery
disease risk assessment is automated given the vessel wall region of the
coronary artery. The results are promising leading to the prototype
design for a clinical setup. [@Arak2016]]{style="color: #8B814C;"}

[**8. Facial recognition with PCA and machine learning
methods**]{style="color: #8B814C;"}

[From the this Paper the authors have identified that the best way to
compare and evaluate the Facial Recognition results with speed and
accuracy was with PCA (Principal Component Analysis) and as well as
Support Vector Machine (SVM). PCA is a to find the principal components
of a given set of images and represent each face image as a smaller size
in lower dimensional face space using the eigenvectors that correspond
to higher eigenvalues. Here the author see's the PCA provides an
expression of an input image in terms of a set of basis images that
appear as "EigenFaces". The eigenfaces can then be used to identify the
individual face from the set of ORL Data Base of Faces. In the SVM a
Given a set of training examples were created and each of them is marked
as belonging to one class. An SVM training algorithm was build to a
model to assign a new example to one class. The SVM model, is a
representation of the examples as points in space and maps these
examples to classes, which are divided by a clear gap as wide as
possible. There New examples can be mapped into the same space and
predicted to belong to the class based on which side of the gap they
fall. Other Mentions were that KNM K Nearest Neighbor another well-known
method of classification was used and that uses each class is a cluster
and the data points belong to a cluster. Where the locations of clusters
need to be determined (cluster center), as well as which data points
belong to which cluster. It finally concludes that based on the three
different methods of facial recognition it showed that SVM/PCA is the
best choice for data with little sensitivity to running speed and a
strong desire to get higher recognition
accuracy.[@Chen2017]]{style="color: #8B814C;"}

[**9. A Comparison of Linear and Non-Linear Machine Learning Techniques
(PCA and SOM) for Characterizing Urban Nutrient
Runoff**]{style="color: #8B814C;"}

[The following Study was based on an analysis of the Linear Machine
learning ( In this Paper we look at , the principal component analysis
(PCA) for the linear technique and the self-organizing map (SOM) for the
nonlinear technique) and supported by Spearman's rank correlation
coefficient (ρ). The Dataset for the models was from a monitoring
campaign was carried out in SB to collect precipitation, flow rate, and
water quality records. Summaries of the observed rainfall-runoff
(antecedent dry period (ADP), total rainfall, event duration, maximum
rainfall intensity, runoff volume, and runoff peak) and water quality
(minimum, maximum, and event mean concentration (EMC)) data are
presented in Tables 1 and 2. Here the main objective of the ML of PCA
and SOM is reducing the dimensionality of a dataset that contains a
large number of interrelated variables while preserving most of the
dataset variance that was collected. Each Data Algorithm was used in
Python3.8 when simulated and The first aspect that was compared between
PCA and SOM was the ability to correlate rainfall characteristics to
water quality variables, in other words, to correctly represent build-up
and wash-off processes To conclude this experiment it was understood
that the primary purpose of this work was the comparison of linear and
non-linear ML techniques, PCA and SOM, respectively, to characterize
urban nutrient runoff. In particular, this comparison was based on three
main aspects: (i) the ability to represent the correlation among the
variables chosen to represent the system and, therefore, depict the
build-up and wash-off processes (cause--effect process) (feature
correlation); (ii) the ability to group the dataset based on the two
variables that symbolize build-up and wash-off processes (data point
grouping); (iii) the ability to identify and quantify the importance of
each variable (feature importance).
[@Gorg2021]]{style="color: #8B814C;"}

[**10. Machine Learning based Multiscale Reduced Kernel PCA for
Nonlinear Process Monitoring**]{style="color: #8B814C;"}

[This paper surround the importance of monitoring Faulty Detections.
Fault detection (FD) is fundamental for monitoring several chemical
processes. Thus, this paper introduces a novel structure multiscale
reduced kernel principal component analysis (MS-RKPCA). Here the paper
aims to address the problem of great computation time and significant
storage memory space by using a data reduction structure based on the
Euclidean distance metric. The paper talks about Process monitoring of
chemical systems is important and essential to observe the best
functioning and to ameliorate the product quality of various industrial
processes. Here it shows Many data-driven techniques have been proposed
and they have been statistically assessed for linearly separable data.
The linearly data driven approaches including principal component
analysis(PCA), and partial least squares (PLS) . These techniques have
been widely applied for dimensionality reduction and they are effective
for modeling and monitoring chemical processes . PCA is extensively
applied for monitoring industrial process. It has proven successful
results for fault detection of the linear process But now However,
popular complex chemical systems exhibit a great nonlinear correlation
between their variables. Then, common chemical systems demand nonlinear
techniques in order to execute tasks that implicate successfully
modeling and monitoring methods Recently, a nonlinear PCA (KPCA) method
has been proposed KPCA technique can efficiently operate the intricate
spatial structure of high-dimensional feature spaces using integral
operators and the notion of kernel tricks . KPCA method purposes to
project the input data onto the feature space by nonlinear kernel
functions and then to perform PCA in that feature space. However most
complex industrial process data are extremely correlated which leads to
a problem when analyzing large process data . Therefore, the first
contribution of this paper is to propose an alternative method called
reduced KPCA (RKPCA) for dealing with the issue of intended storage
space, and elevated computation time. The improved RKPCA method consists
of using the Euclidean distance to keep an observation in case of
redundancy between observations. It is noted Thus, RKPCA accelerates the
evaluation of the test sample and also saves the memory of keeping the
trained sample. The obtained reduced data contains the most relevant
information about the dataset. To be effective, any FD technique must
rely on the goodness of the process data. To conclude , a technique that
merges a size reduction framework and wavelet analysis is proposed to
monitor nonlinear processes and enhance the fault detection ability of
conventional KPCA. The key view of the developed technique is to apply
the multiscale representation on the reduced dataset using the Euclidean
distance metric in order to improve the modeling abilities and provide
good effectiveness over the KPCA model. The Tennessee Eastman Process
(TEP) data is used to validate the effectiveness of the develop method.
The results demonstrate the effectiveness of the proposed multiscale
reduced KPCA (MS-RKPCA) approach over both RKPCA and KPCA models to
increase the computation times and to ensure the quality of the
detection (MDR and FAR).[@Dhibi2020]]{style="color: #8B814C;"}

[**11. Principal Component Analysis**]{style="color: #8B814C;"}

[This is a brief article discussing the accumulation of error when
applying PCA to large datasets and how to locate important patterns when
using the method and how scaling can affect the dataset being analyzed.
It provides several graphs and
figures.[@Lever]]{style="color: #8B814C;"}

[**12. PCAtest: Testing the Statistical Significance of Principal
Component Analysis in R**]{style="color: #8B814C;"}

[This article begins with a brief history of PCA and then moves on to
discuss an R package known as PCAtest which implements permutation-based
statistical tests to evaluate the overall significance of a PCA. The
article describes the process that PCAtest implements: a bootstrapping
procedure that calculates various statistics related to PCA. From there
the article discusses the results of an example dataset that has been
analyzed using PCA and then that PCA analyzed using PCAtest. Visuals and
graphical aids are included. Finally the article concludes by discussing
when to use PCA and how to interpret the results.
[@Camargo]]{style="color: #8B814C;"}

[**13. Principal Component Analysis**]{style="color: #8B814C;"}

[This article is a brief overview of PCA and describes the methods used
throughout the process of performing PCA while mentioning its
application in facial recognition software and then mentioning some
advantages and disadvantages of using the method.
[@Sasan]]{style="color: #8B814C;"}

[**14. Exploration of Principal Component Analysis: Deriving Principal
Component Analysis Visually Using Spectra**]{style="color: #8B814C;"}

[This article introduces PCA and then goes on to describe methods and
providing examples of PCA while using spectral graphs as visual aids. A
brief description of the matrix theory behind PCA is analyzed through
the use of spectral graphs and finally instructions on how to interpret
results from PCA is given as a general model.
[@Beattie]]{style="color: #8B814C;"}

[**15. Principal Component Analysis: A Review and Recent
Developments**]{style="color: #8B814C;"}

[This article describes PCA's usefulness in data science and in dealing
with large datasets and from there gives a basic description of the
methods involved in PCA followed by an example. The article then
proceeds to discuss some issues related to PCA while providing plots and
visuals and then examines some adaptations of PCA with examples
included. [@Jolliffe]]{style="color: #8B814C;"}

## [Methods]{style="color: #191970;"}

## [Analysis and Results]{style="color: #191970;"}

## [Data and Vizualisation]{style="color: #191970;"}

[Bacteria dataset from MASS package (220 rows by 6
columns)]{style="color: #8B814C;"}

[Dr A. Leach tested the effects of a drug on 50 children with a history
of otitis media in the Northern Territory of Australia. The children
were randomized to the drug or the a placebo, and also to receive active
encouragement to comply with taking the drug. The presence of H.
influenzae was checked at weeks 0, 2, 4, 6 and 11: 30 of the checks were
missing and are not included in this data
frame.]{style="color: #8B814C;"}

[**Variables**]{style="color: #8B814C;"}

[**y** - presence or absence: a factor with levels n and
y]{style="color: #8B814C;"}

[**ap** - active/placebo: a factor with levels a and
p]{style="color: #8B814C;"}

[**hilo** - hi/low compliance: a factor with levels hi amd
lo]{style="color: #8B814C;"}

[**week** - numeric: week of test]{style="color: #8B814C;"}

[**ID** - subject ID: a factor]{style="color: #8B814C;"}

[**trt** - a factor with levels placebo, drug and drug+, a re-coding of
ap and hilo]{style="color: #8B814C;"}

```{r, warning=FALSE, echo=T, message=FALSE}
# loading packages 
library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(MASS)

```

```{r, warning=FALSE, echo=TRUE}
# Load Data
kable(head(bacteria))

#ggplot1 = murders %>% ggplot(mapping = aes(x=population/10^6, y=total)) 

  #ggplot1 + geom_point(aes(col=region), size = 4) +
  #geom_text_repel(aes(label=abb)) +
  #scale_x_log10() +
  #scale_y_log10() +
  #geom_smooth(formula = "y~x", method=lm,se = F)+
  #xlab("Populations in millions (log10 scale)") + 
  #ylab("Total number of murders (log10 scale)") +
  #ggtitle("US Gun Murders in 2010") +
  #scale_color_discrete(name = "Region")+
  #    theme_wsj()

```

## [Statistical Modeling]{style="color: #191970;"}

## [Conlusion]{style="color: #191970;"}

## [References]{style="color: #191970;"}
