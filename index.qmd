---
title: "PCA (principal component analysis)"
author: 
- Brandy Carr
- Lee
- Tavish
date: '`r Sys.Date()`'
format:
  html:
    highlight: pygments
    anchor-sections: false
    smooth-scroll: true
    toc: true
    mainfont: sans-serif
    fontcolor: "#8B814C"
    monobackgroundcolor: "#FAFAF5"
    backgroundcolor: "#FAFAF5"
    headercolor: "#191970"
    code-line-numbers: true
    code-block-bg: "FFFFFF"
    code-fold: true
    code-background: true
    cap-location: bottom
    margin-left: "0in"
    margin-right: "0in"
    fig-width: 12
    fig-height: 8
execute: 
  warning: false
  echo: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
number-sections: true
editor: 
  markdown:
    wrap: 100
---

```{=html}
<style>
details > summary {
  padding: 1px;
  color: #8B814C;
}
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 body {
    color: #8B814C;
    background-color: #FAFAF5;
    font-size: 12pt;
    font-weight: lighter;
  }
  h1 {
    color: #191970;
    font-weight: bold;
}
  h2 {
    color: #191970;  
}
  h3 {
    color: #191970;  
}
  h4 {
    color: #191970;  
}
 .bibliography {
    background-color: #FAFAF5;
}
 .reference-section-title {
    color: #191970;
}
</style>
```
## Introduction

-   Explains the background of research on a topic.
-   Demonstrates why a topic is significant to a subject area.
-   Discovers relationships between research studies/ideas.
-   Identifies major themes, concepts, and researchers on a topic.
-   Identifies critical gaps and points of disagreement.
-   Discusses further research questions that logically come out of the previous studies.

Principal Component Analysis (PCA) is an unsupervised machine learning method (@fig-machineLearning)
that can be used as the first step in data analysis. The main goal of PCA is to reduce the number of
dimensions/features while retaining most of the original variability. Reducing the number of
features makes it easier to graphically visualize hidden patterns in the data, it's also useful when
encountering a high number of predictor variables or highly correlated variables when running a
regression model [@lang2021applied]. Using a clustering algorithm, such as k-means, after the data
has been transformed with PCA is a powerful method to use on high dimensional data containing
linearly correlated features [@Alkha].

The most important use of PCA is to represent a multivariate data table as smaller set of variables
(summary indices) to observe hidden trends, jumps, clusters, patterns and outliers.

PCA is a broadly used statistical method whose use stretches across many scientific disciplines, and
in turn many different adaptations of PCA have been developed based on the variation in goals and
data types associated with each respective discipline. PCA was developed first by Pearson (1901) and
Hotelling (1933) [@Camargo]. This technique transforms some number of possibly correlated variables
into a smaller number of variables, the variables in this smaller matrix are referred to as the
Principal Components (PC). This is achieved using a vector space transform. By mathematical
projection we can interpret the original data set with just a few variables, namely the principal
components achieved through calculation. The purpose of reducing dimension size in large data sets
is to make it easier to spot trends, patterns, and outliers in data where that information would
have previously been hidden by the size of data set of interest (Richardson). The information being
preserved in the process of reduction is the variability in the data (i.e. the statistical
information present in the data). In order to preserve as much variability as possible we should
"...find new variables that are linear functions of those in the original data set, that
successively maximize variance and that are uncorrelated with each other" [@Jolliffe]. PCA assumes
no distribution in data in a descriptive context which is one of it's key features that makes it so
widely adaptive as an exploratory method across disciplines and data types [@Jolliffe]. To lists all
of PCA's applications would be tedious and excessive, but some examples where PCA has been used is
in facial recognition, image analysis, analysis of web data, and cyber security analysis.
Essentially anywhere that large data sets are found PCA can be used to aid in discovering trends
amongst the variables of that data. One key field PCA has seen use is in the analysis of cyber
security network data. PCA could see many applications in web data analysis as the amount of data
for this purpose is always increasing and becoming more relevant.

Principal Component Analysis and Machine Learning Methods. In our research, we learn the
applications of how important and groundbreaking Principal Component Analysis and Machine Learning
Methods are. In these papers we see how promising the impact is of PCA with recognizing Microanerysm
in medicine and how groundbreaking has been critical for diagnosis and treatment of Diabetic
Retinopathy [@Cao2018]. Other research papers show a framework for coronary artery disease risk
assessment in intravascular ultrasound. The paper reflects on a novel strategy for risk
stratification based on plaque morphology embedded with principal component analysis (PCA) for
plaque feature dimensionality reduction and dominant feature selection technique.[@Gorg2021] Facial
recognition with PCA and machine learning methods, From this Paper the authors have identified that
the best way to compare and evaluate the Facial Recognition results with speed and accuracy was with
PCA (Principal Component Analysis) and as well as Support Vector Machine (SVM). Lastly we see the
importance of PCA with it identifying that the best way to compare and evaluate the Facial
Recognition results with speed and accuracy was with PCA (Principal Component Analysis) and as well
as Support Vector Machine (SVM)[@Camargo].

Taking a look into a real world example, say we have a dataset consisting of 1,000 students with
exam scores from 7 different courses: Advanced Statistics, Probability Theory, Intro to Dance, World
Religions, and Religion in America. We could group Advanced Statistics and Probability Theory into a
new variable called Stats, group World Religions and Religion in America into a new variable
Religion, and keep Intro to Dance by itself. We have Reduced the data set from 7 variables to 3
without much loss in variation. This is the main concept behind PCA except variables are not
manually regrouped but instead the new variables (principal components) are derived from certain
linear combinations of the original variables [@lang2021applied].

PCA can also be useful when studying mental disorders, data consisting of symptoms and the
connections between them. When many symptoms are being observed, it can be difficult to visually
represent the connections between them, both the strength of the connections and the proximity to
each other. Plotting using the first 2 principal components allows for the placement on the x or y
axis to become interpretable, that is, observations far left differ in some dimension (the first
principal component) compared to ones far right. The same can be said in the y direction [@Jones].

![Machine Learning](index_files/figure-html/machineLearning_fig1.png){#fig-machineLearning}

## Methods

PCA forms the basis of multivariate data analysis based on projection methods. The variability in
the original (correlated) variables will be explained through a smaller (uncorrelated) set of new
variables i.e. the principal components (PC's). PCA results depend on the scale or units of the
variables so, for unscaled data, calculations should either be preformed on the correlation matrix
(instead of the covariance matrix) or the data should be standardized with mean 0 and variance 1 (z
scores) [@lang2021applied].

```{mermaid, code-fold: hide}

%%{init: {'theme': 'base', 'themeVariables': { 'background': '#FAFAF5', 'primaryColor': '#EAEAD6', 'nodeBorder': '#8B814C', 'lineColor': '#8B814C', 'primaryTextColor': '#191970', 'textColor': '#191970', 'fontSize': '12px', 'width': '100%'}}}%%

flowchart LR
A[(Clean<br/>Data)] --> B((Are all<br/>vars the same<br/>scale/unit?))
B --> C((Yes))
B --> D((No))
D -.- |Standardize<br/>Data| E(Estimate<br/>Sample<br/>Mean<br/>Vector)
C --> E
D --> F(Estimate<br/>Sample<br/>Mean<br/>Vector)
E --> G(Estimate<br/>Sample<br/>Covariance<br/>Matrix)
F --> H(Estimate<br/>Sample<br/>Correlation<br/>Matrix)
G --> I(Eigenvalues<br/>Eigenvectors)
H --> I

```

### Assumptions

-   Variables should be continuous, interval or ratio level of measurement (ordinal variables, such
    as likert-scale, are also often used)
-   Variables should all be the same scale/units (@eq-stdzData)
-   Variables are linearly related - visually check scatter plot matrices (see @sec-linearity for an
    example)
-   Outliers & missing or impossible values should be removed (see @sec-cleanData for an example)

### Sample Data

```{=html}
<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  overflow:hidden;padding:10px 8px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  font-weight:normal;overflow:hidden;padding:10px 8px;word-break:normal;}
.tg .tg-vi6l{background-color:#ffffff;border-color:#8b814c;color:#8b814c;text-align:center;vertical-align:top}
.tg .tg-pi0b{background-color:#eaead6;border-color:#8b814c;color:#8b814c;font-weight:bold;text-align:center;vertical-align:top}
</style>
```
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
|                                                                                                                            | \$X_1\$                                                                                                     | \$X_2\$                                                                                                     | \...                                                                                                        | \$X_j\$                                                                                                     | \...                                                                                                        | \$X_p\$                                                                                                     |
+============================================================================================================================+=============================================================================================================+=============================================================================================================+=============================================================================================================+=============================================================================================================+=============================================================================================================+=============================================================================================================+
| \$x_1\$                                                                                                                    | \$x\_{11}\$                                                                                                 | \$x\_{12}\$                                                                                                 | \...                                                                                                        | \$x\_{1j}\$                                                                                                 | \...                                                                                                        | \$x\_{1p}\$                                                                                                 |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
| \$x_2\$                                                                                                                    | \$x\_{21}\$                                                                                                 | \$x\_{22}\$                                                                                                 | \...                                                                                                        | \$x\_{2j}\$                                                                                                 | \...                                                                                                        | \$x\_{2p}\$                                                                                                 |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
| \...                                                                                                                       | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
| \$x_i\$                                                                                                                    | \$x\_{i1}\$                                                                                                 | \$x\_{i2}\$                                                                                                 | \...                                                                                                        | \$x\_{ij}\$                                                                                                 | \...                                                                                                        | \$x\_{ip}\$                                                                                                 |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
| \...                                                                                                                       | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        | \...                                                                                                        |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+
| \$x_n\$                                                                                                                    | \$x\_{n1}\$                                                                                                 | \$x\_{n2}\$                                                                                                 | \...                                                                                                        | \$x\_{nj}\$                                                                                                 | \...                                                                                                        | \$x\_{np}\$                                                                                                 |
+----------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------+

### Equations

**Standardized Data:**
$$z_{ij}\ =\ \frac{x_{ij} - \overline{x_j}}{\sqrt{\hat{{\sigma}_{jj}}}}, \ \ \ \ \ where\ \ \ \ i\ =\ \{1,2,...,n\}, \ \ \ \ j\ =\ \{1,2,...,p\}$$ {#eq-stdzData}

**Random Vector:**
$${x_i} = {({x_{i1}}, ... , {x_{ip}})}^T,\ \ \ \ \ \ \ \ \ where\ \ i\ =\ \{1,2,...,n\}$$ {#eq-randomVector}

**Sample Mean Vector:**
$$\hat{\mu}\ \ =\ \ \overline{x}\ \ =\ \ \frac{1}{n} \sum_{i=1}^{n} x_i$$ {#eq-meanVector}

**Sample Covariance Matrix:**
$$\hat{\sum}\ \ =\ \ S\ \ =\ \ {(\hat{\sigma}_{ij})}_{p{\sf x}p}\ \ =\ \ \frac{1}{n-1} \sum_{i=1}^{n} {(x_i - \overline{x})(x_i - \overline{x})}^T$$ {#eq-covMatrix}

::: {.callout-note collapse="true"}
[The **sample mean vector** represents the center of the random vector
($x_i$)]{style="color: #1A79E1; font-size:12px;"}

[The **sample covariance matrix** represents the variations (diagonal elements) & correlations
(off-diagonal elements) of the random vector ($x_i$)]{style="color: #1A79E1; font-size:12px;"}
:::

**Eigenvectors:** $${\hat{a}_{k}}$$ {#eq-eigenvec}

**Eigenvalues:**
$${\hat{y}_{ik}}\ =\ {\hat{a}^T_{k}}(x_i\ -\ \overline{x}),\ \ \ \ \ \ \ i\ =\ \{1,2,...,n\},\ \ \ \ \ \ \ k\ =\ \{1,2,...,p\}$$ {#eq-eigenval}

<br>

## Analysis and Results

### Packages

```{r, packages}
library(dplyr)
library(ggplot2)
library(data.table)
library(ggfortify)
library(MASS)
library(tidyr)
library(paletteer)
library(knitr)
library(DescTools)
```

### Data

Add description of variables & brief background on how data was collected etc

#### Read & Clean Data {#sec-cleanData}

```{r}
# READ DATA FROM A GITHUB CSV FILE
conspiracy<- (read.csv("https://raw.githubusercontent.com/bjcarr08/sampleData/main/kaggleConspiracyTheoriesData.csv", stringsAsFactors = T))[,-1]

# REMOVE ROWS WITH NAs & IMPOSSIBLE VALUES (removed rows where participant marked 'not sure' as political ideology)
conspiracy<- conspiracy[complete.cases(conspiracy),] %>% filter(y!="Not Sure")
```

#### Visualize Data {#sec-linearity}

```{r, plot0, fig.height=12, fig.width=12}
# SCATTER PLOT MATRICES: TO CHECK LINEARITY ASSUMPTION
par(col.axis="#8B814C",col.lab="#8B814C",col.main="#8B814C",col.sub="#8B814C",pch=20, col="#8B814C", bg="transparent")
PlotPairs(sapply(conspiracy[,-9], function(x) jitter(x, 5)), 
                     g=conspiracy$y,
                     col=alpha("#8B814C", 0.1), 
                     col.smooth="#8B814C")
```

```{r, plot1, fig.height=6, fig.width=12}
# TRANSFORM TO LONG DATA FOR PLOTS
conspiracy.Long<- conspiracy %>% pivot_longer(!y, names_to="conspiracy", values_to="score", values_transform=list(score=as.numeric))

# HISTOGRAMS
ggplot(conspiracy.Long, aes(score, fill=conspiracy, color=conspiracy)) +
  geom_histogram(alpha=0.2, breaks=seq(0,5,1)) +
  lemon::facet_rep_wrap(.~conspiracy, nrow=2, labeller="label_both", repeat.tick.labels=T) +
  labs(title="Raw Scores") +
  theme_bw() +
  theme(legend.position = "none",
        panel.border = element_rect(color = "#8B814C"),
        strip.background = element_rect(fill = "#EAEAD6", color = "#8B814C"),
        strip.text = element_text(color = "#8B814C"),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C"),
        axis.title = element_text(color = "#8B814C"),
        plot.title = element_text(color = "#8B814C"),
        axis.ticks = element_line(color = "#8B814C"))
```

### PCA

```{r}
options(width = 100)

# STANDARDIZE DATA
#conspiracy.Stdz<- conspiracy %>% mutate(across(.cols=truther911:vaportrail, scale))
#conspiracy<- conspiracy %>% mutate(across(.cols=truther911:vaportrail, scale))

# RE-LEVEL POLITICAL IDEOLOGY (Very Liberal - Very Conservative)
conspiracy$y<- factor(conspiracy$y, levels=c("Very Liberal", "Liberal", "Somewhat Liberal", "Middle of the Road", "Somewhat Conservative", "Conservative", "Very Conservative"))

# RE-NAMED VARIABLE 'y'
names(conspiracy)[9]<- "PoliticalIdeology"

# DATA FOR PCA FUNCTION (only keep numeric variables)
df<- conspiracy[,-9]

# PCA
#pc1<- prcomp(df, scale.=T)
pc1<- prcomp(df)

summary(pc1)
```

#### Scree-Plot

```{r}
plot(pc1, type="line", bg="transparent", col="#8B814C")
```

#### Biplot

```{r}
autoplot(pc1,
  # AUTOPLOT OPTIONS
  data=conspiracy, 
  colour="PoliticalIdeology", 
  loadings=T, loadings.colour=alpha("#191970", 0.5), 
  loadings.label=T, loadings.label.colour="#191970", loadings.label.size=5, loadings.label.hjust=0) + 
  # CUSTOM COLORS FOR POLITICAL IDEOLOGY GROUPS
  scale_colour_manual(values = alpha(paletteer_d("rcartocolor::Temps"), 0.5)) +
  # GGPLOT THEME OPTIONS
  theme_bw() +
  theme(legend.key = element_rect(fill = "#FAFAF5"),
        legend.background = element_rect(fill = "#FAFAF5"),
        legend.text = element_text(color = "#8B814C", size = 14),
        legend.title = element_text(color = "#8B814C", size = 16),
        panel.border = element_rect(color = "#8B814C"),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C", size = 14),
        axis.title = element_text(color = "#8B814C", size = 16),
        axis.ticks = element_line(color = "#8B814C"))
```

#### Loadings

The matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors).

```{r}
(princomp(df))$loadings
```

## Conclusion

## References {.bibliography style="background-color: #FAFAF5; color: #8B814C"}
