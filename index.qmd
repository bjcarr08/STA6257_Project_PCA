---
title: "PCA (principal component analysis)"
authors: Brandy Carr, Lee, Tavish
date: '`r Sys.Date()`'
format:
  html:
    highlight: pygments
    anchor-sections: false
    smooth-scroll: true
    toc: true
    toc-depth: 3
    number-depth: 2
    mainfont: sans-serif
    fontcolor: "#8B814C"
    monobackgroundcolor: "#FAFAF5"
    backgroundcolor: "#FAFAF5"
    headercolor: "#191970"
    code-line-numbers: true
    code-block-bg: "FFFFFF"
    code-fold: show
    code-background: true
    cap-location: top
    margin-left: "0in"
    margin-right: "0in"
    fig-width: 12
    fig-height: 8
execute: 
  warning: false
  echo: true
website:
  navbar:
    background: "#191970"
    search: false
    collapse: false
    pinned: true
    left:
      - index.qmd
      - slides.qmd
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
number-sections: true
editor: 
  markdown:
    wrap: 100
---

[Website](https://bjcarr08.github.io/STA6257_Project_PCA/)

[Slides](slides.html)

<style>
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 body {
    color: #8B814C;
    background-color: #FAFAF5;
    font-size: 12pt;
    font-weight: lighter;
  }
  h1 {
    color: #191970;
    font-weight: bold;
}
  h2 {
    color: #191970;  
}
  h3 {
    color: #191970;  
}
  h4 {
    color: #191970;  
}
 .bibliography {
    background-color: #FAFAF5;
}
 .reference-section-title {
    color: #191970;
}
</style>


## Introduction

Principal Component Analysis (PCA) is an unsupervised machine learning method (@fig-machineLearning)
that can be used as the first step in data analysis. The main goal of PCA is to reduce the number of
dimensions/features while retaining most of the original variability. Reducing the number of
features makes it easier to graphically visualize hidden patterns in the data, it's also useful when
encountering a high number of predictor variables or highly correlated variables when running a
regression model [@lang2021applied]. After the data has been transformed with PCA, using a clustering 
algorithm such as k-means can provide additional power to the analysis of high dimensional data containing 
linearly correlated features [@Alkha].

The most important use of PCA is to represent a multivariate data table as smaller set of variables
(summary indices) to observe hidden trends, jumps, clusters, patterns and outliers.

PCA is a broadly used statistical method whose use stretches across many scientific disciplines, and
in turn many different adaptations of PCA have been developed based on the variation in goals and
data types associated with each respective discipline. PCA was developed first by Pearson (1901) and
Hotelling (1933) [@Camargo]. This technique transforms some number of possibly correlated variables
into a smaller number of variables, the variables in this smaller matrix are referred to as the
Principal Components (PC). This is achieved using a vector space transform. By mathematical
projection we can interpret the original data set with just a few variables, namely the principal
components achieved through calculation. Reducing dimension size in large data sets makes it easier to 
spot trends, patterns, and outliers in data where that information would have previously been hidden by 
the size of data (Richardson). The information being preserved in the process of reduction is the 
variability in the data (i.e. the statistical information present in the data). In order to preserve as 
much variability as possible we should
"...find new variables that are linear functions of those in the original data set, that
successively maximize variance and that are uncorrelated with each other" [@Jolliffe]. PCA assumes
no distribution in data in a descriptive context, one of it's key features that makes it so
widely adaptive as an exploratory method across disciplines and data types [@Jolliffe]. 

To lists all of PCA's applications would be tedious and excessive, but some examples where PCA has been 
used is in image analysis, analysis of web data, and cyber security analysis.
Essentially anywhere that large data sets are found PCA can be used to aid in discovering trends
amongst the variables of that data. PCA can also be useful when studying mental disorders, data consisting 
of symptoms and the connections between them. When many symptoms are being observed, it can be difficult to visually
represent the connections between them, both the strength of the connections and the proximity to
each other. Plotting using the first 2 principal components allows for the placement on the x or y
axis to become interpretable, that is, observations far left differ in some dimension (the first
principal component) compared to ones far right. The same can be said in the y direction [@Jones]. PCA also 
aids in recognizing microanerysm in medicine and how groundbreaking has been critical for diagnosis and treatment of Diabetic
retinopathy [@Cao2018]. Research papers show a framework for coronary artery disease risk
assessment in intravascular ultrasound. The paper reflects on a novel strategy for risk
stratification based on plaque morphology embedded with principal component analysis (PCA) for
plaque feature dimensionality reduction and dominant feature selection technique[@Gorg2021]. Camargo identified that
the best way to compare and evaluate facial recognition results with speed and accuracy is with
PCA (Principal Component Analysis), alongside Support Vector Machine (SVM) methods [@Camargo].

Taking a look into a real world example, say we have a dataset consisting of 1,000 students with
exam scores from 7 different courses: Advanced Statistics, Probability Theory, Intro to Dance, World
Religions, and Religion in America. We could group Advanced Statistics and Probability Theory into a
new variable called Stats, group World Religions and Religion in America into a new variable
Religion, and keep Intro to Dance by itself. We have Reduced the data set from 7 variables to 3
without much loss in variation. This is the main concept behind PCA except variables are not
manually regrouped but instead the new variables (principal components) are derived from certain
linear combinations of the original variables [@lang2021applied].

![Machine Learning](index_files/figure-html/machineLearning_fig1.png){#fig-machineLearning}

## Methods

PCA forms the basis of multivariate data analysis based on projection methods. The variability in
the original (correlated) variables will be explained through a smaller (uncorrelated) set of new
variables i.e. the principal components (PC's). PCA results depend on the scale or units of the
variables so, for unscaled data, calculations should either be preformed on the correlation matrix
(instead of the covariance matrix) or the data should be standardized with mean 0 and variance 1 (z
scores) [@lang2021applied]. Firstly, when performing PCA, a new set of orthogonal coordinate axes are 
identified from the original data set, which is accomplished by finding the direction of maximal variance 
through each dimension. This is equivalent to using the least squares method to find the line of best fit. 
This new axis is the first principal component of the data set. Next we use orthogonal projection to 
project the coordinates onto the new axis. Once this is done we obtain a second principal component 
(and principal coordinate axis) by finding the direction of the second largest variance in the data, this 
axis is orthogonal to our first PC. These two PCs define a plane onto which we can project further coordinates onto (Richardson).

```{mermaid, flowchart, code-fold: hide}

%%{init: {'format': 'svg', 'theme': 'base', 'themeVariables': { 'background': '#FAFAF5', 'primaryColor': '#EAEAD6', 'nodeBorder': '#8B814C', 'lineColor': '#8B814C', 'primaryTextColor': '#191970', 'textColor': '#191970', 'fontSize': '12px', 'width': '100%'}}}%%

flowchart LR
A[(Clean<br/>Data)] --> B((Are all<br/>vars the same<br/>scale/unit?))
B --> C((Yes))
B --> D((No))
D -.- |Standardize<br/>Data| E(Estimate<br/>Sample<br/>Mean<br/>Vector)
C --> E
D --> F(Estimate<br/>Sample<br/>Mean<br/>Vector)
E --> G(Estimate<br/>Sample<br/>Covariance<br/>Matrix)
F --> H(Estimate<br/>Sample<br/>Correlation<br/>Matrix)
G --> I(Eigenvalues<br/>Eigenvectors)
H --> I

```

### Assumptions

-   Variables should be continuous, interval or ratio level of measurement (ordinal variables, such
    as likert-scale, are also often used)
-   Variables should all be the same scale/units (@eq-stdzData)
-   Variables are linearly related - visually check scatter plot matrices (see @sec-linearity for an
    example)
-   Outliers & missing or impossible values should be removed (see @sec-cleanData for an example)

### Sample Data

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  overflow:hidden;padding:10px 8px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  font-weight:normal;overflow:hidden;padding:10px 8px;word-break:normal;}
.tg .tg-vi6l{background-color:#ffffff;border-color:#8b814c;color:#8b814c;text-align:center;vertical-align:top}
.tg .tg-pi0b{background-color:#eaead6;border-color:#8b814c;color:#8b814c;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 791px">
<colgroup>
<col style="width: 126px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
</colgroup>
<thead>
  <tr>
    <th class="tg-pi0b"></th>
    <th class="tg-pi0b">$X_1$</th>
    <th class="tg-pi0b">$X_2$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_j$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_p$</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-pi0b">$x_1$</td>
    <td class="tg-vi6l">$x_{11}$</td>
    <td class="tg-vi6l">$x_{12}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_2$</td>
    <td class="tg-vi6l">$x_{21}$</td>
    <td class="tg-vi6l">$x_{22}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_i$</td>
    <td class="tg-vi6l">$x_{i1}$</td>
    <td class="tg-vi6l">$x_{i2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ij}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ip}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_n$</td>
    <td class="tg-vi6l">$x_{n1}$</td>
    <td class="tg-vi6l">$x_{n2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{nj}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{np}$</td>
  </tr>
</tbody>
</table>

### Equations

**Standardized Data:** $${z_{ij}}\ =\ \frac{{x_{ij}} - {\overline{x_j}}} {\sqrt{\hat{\sigma_{jj}}}}, \ \ \ \ \ where\ \ \ \ i\ =\ \{1,2,...,n\}, \ \ \ \ j\ =\ \{1,2,...,p\}$$ {#eq-stdzData}

**Random Vector:** $${x_i} = {({x_{i1}}, ... , {x_{ip}})}^T,\ \ \ \ \ \ \ \ \ where\ \ i\ =\ \{1,2,...,n\}$$ {#eq-randomVector}

**Sample Mean Vector:** $$\hat{\mu}\ \ =\ \ \overline{x}\ \ =\ \ \frac{1}{n} \sum_{i=1}^{n} x_i$$ {#eq-meanVector}

**Sample Covariance Matrix:** $$\hat{\sum}\ \ =\ \ S\ \ =\ \ {(\hat{\sigma}_{ij})}_{p{\sf x}p}\ \ =\ \ \frac{1}{n-1} \sum_{i=1}^{n} {(x_i - \overline{x})(x_i - \overline{x})}^T$$ {#eq-covMatrix}

::: {.callout-note collapse="true"}
[The **sample mean vector** represents the center of the random vector
($x_i$)]{style="color: #1A79E1; font-size:12px;"}

[The **sample covariance matrix** represents the variations (diagonal elements) & correlations
(off-diagonal elements) of the random vector ($x_i$)]{style="color: #1A79E1; font-size:12px;"}
:::

**Eigenvectors:** $${\hat{a}_{k}}$$ {#eq-eigenvec}

**Eigenvalues:** $${\hat{y}_{ik}}\ =\ {\hat{a}^T_{k}}(x_i\ -\ \overline{x}),\ \ \ \ \ \ \ i\ =\ \{1,2,...,n\},\ \ \ \ \ \ \ k\ =\ \{1,2,...,p\}$$ {#eq-eigenval}

<br>

## Analysis and Results

### Packages

```{r, packages}
library(dplyr)
library(ggplot2)
library(data.table)
library(ggfortify)
library(MASS)
library(tidyr)
library(paletteer)
library(knitr)
library(DescTools)
library(gt)
library(gridExtra)
```

### Data

Results of 2000 respondents answering a 5-point Likert scale of their belief in various conspiracies. Sampled from a nationally representative survey in 2011. (Replication data and code may be found at the AJPS Dataverse website - <a>https://thedata.harvard.edu/dvn/dv/ajps</a>) [@originalConspiracyData]


**SCALE LEVELS:**

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  overflow:hidden;padding:10px 5px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:14px;
  font-weight:normal;overflow:hidden;padding:10px 5px;word-break:normal;}
.tg .tg-baqh{text-align:center;vertical-align:top}
.tg .tg-amwm{font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 258px">
<colgroup>
<col style="width: 163px">
<col style="width: 95px">
</colgroup>
<thead>
  <tr>
    <th class="tg-amwm">Label</th>
    <th class="tg-amwm">Level</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-baqh">Strongly Disagree</td>
    <td class="tg-baqh">1</td>
  </tr>
  <tr>
    <td class="tg-baqh">Disagree</td>
    <td class="tg-baqh">2</td>
  </tr>
  <tr>
    <td class="tg-baqh">Neither</td>
    <td class="tg-baqh">3</td>
  </tr>
  <tr>
    <td class="tg-baqh">Agree</td>
    <td class="tg-baqh">4</td>
  </tr>
  <tr>
    <td class="tg-baqh">Strongly Agree</td>
    <td class="tg-baqh">5</td>
  </tr>
</tbody>
</table>


**VARIABLES:**

- **truther911:** Certain U.S. government officials planned the attacks of September 11, 2001, because they wanted the United States to go to war in the Middle East.
- **obamabirth:** President Barack Obama was not really born in the United States and does not have an authentic Hawaiian birth certificate.
- **fincrisis:** The current financial crisis was secretly orchestrated by a small group of Wall Street bankers to extend the power of the Federal Reserve and further their control of the world's economy.
- **flourolights:** The U.S. government is mandating the switch to compact fluorescent light bulbs because such lights make people more obedient and easier to control.
- **endtimes:** We are currently living in End Times as foretold by Biblical prophecy.
- **sorosplot:** Billionaire George Soros is behind a hidden plot to destabilize the American government, take control of the media, and put the world under his control.
- **iraqjews:** The U.S. invasion of Iraq was not part of a campaign to fight terrorism, but was driven by oil companies and Jews in the U.S. and Israel.
- **vaportrail:** Vapor trails left by aircraft are actually chemical agents deliberately sprayed in a clandestine program directed by government officials.


**GROUPING FACTOR (y = political ideology):**

- Very Liberal
- Liberal
- Somewhat Liberal
- Middle of the Road
- Somewhat Conservative
- Conservative
- Very Conservative


#### Read & Clean Data {#sec-cleanData}

```{r}
# READ DATA FROM A GITHUB CSV FILE
conspiracy<- (read.csv("https://raw.githubusercontent.com/bjcarr08/sampleData/main/kaggleConspiracyTheoriesData.csv", stringsAsFactors = T))[,-1]

# REMOVE ROWS WITH NAs & IMPOSSIBLE VALUES (removed rows where participant marked 'not sure' as political ideology)
conspiracy<- conspiracy[complete.cases(conspiracy),] %>% filter(y!="Not Sure")
```

```{r, first10rows, echo=FALSE, codefold="hide"}
#| label: fig-first10rows
#| fig-cap: "First 10 Rows of Conspiracy Data"

png("~/Git/STA6257_Project_PCA/index_files/figure-html/first10rows.png", height=550, width=1000)
p<-tableGrob(conspiracy[c(1:10),])
grid.arrange(p)
dev.off()
```

#### Visualize Data {#sec-linearity}

Visually checking if linearity between variables assumption is met (not a strictly help assumption, especially when using ordinal data).

The correlation matrix plots show enough of a relationship between variables to meet the assumption of linearity.

```{r, plot0, fig.height=12, fig.width=12}
# SCATTER PLOT MATRICES: TO CHECK LINEARITY ASSUMPTION
par(col.axis="#8B814C",col.lab="#8B814C",col.main="#8B814C",col.sub="#8B814C",pch=20, col="#8B814C", bg="transparent")
DescTools::PlotPairs(sapply(conspiracy[,-9], function(x) jitter(x, 5)), 
                     g=conspiracy$y,
                     col=alpha("#8B814C", 0.1), 
                     col.smooth="#8B814C")
```

```{r, plot1, fig.height=6, fig.width=12}
# TRANSFORM TO LONG DATA FOR PLOTS
conspiracy.Long<- conspiracy %>% pivot_longer(!y, names_to="conspiracy", values_to="score", values_transform=list(score=as.numeric))

# HISTOGRAMS
ggplot(conspiracy.Long, aes(score, fill=conspiracy, color=conspiracy)) +
  geom_histogram(alpha=0.2, breaks=seq(0,5,1)) +
  lemon::facet_rep_wrap(.~conspiracy, nrow=2, labeller="label_both", repeat.tick.labels=T) +
  labs(title="Distributions of Raw Score") +
  theme_bw() +
  theme(legend.position = "none",
        panel.border = element_rect(color = "#8B814C"),
        strip.background = element_rect(fill = "#EAEAD6", color = "#8B814C"),
        strip.text = element_text(color = "#8B814C", size=14),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C"),
        axis.title = element_text(color = "#8B814C", size=14),
        plot.title = element_text(color = "#8B814C", size=14),
        axis.ticks = element_line(color = "#8B814C"))
```

Looking at the distribution plots above, we can see that all variables are measured in the same scale/units & do not need to be standardized.

### PCA

```{r}
options(width = 100)

# STANDARDIZE DATA [SKIP]
#conspiracy<- conspiracy %>% mutate(across(.cols=truther911:vaportrail, scale))

# RE-LEVEL POLITICAL IDEOLOGY (Very Liberal - Very Conservative)
conspiracy$y<- factor(conspiracy$y, levels=c("Very Liberal", "Liberal", "Somewhat Liberal", "Middle of the Road", "Somewhat Conservative", "Conservative", "Very Conservative"))

# RE-NAMED VARIABLE 'y'
names(conspiracy)[9]<- "PoliticalIdeology"

# DATA FOR PCA FUNCTION (only keep numeric variables)
df<- conspiracy[,-9]

# PCA
#pc1<- prcomp(df, scale.=T)
pc1<- prcomp(df)

summary(pc1)
```

From the table directly above (Importance of components), we are mainly focused on the first 2 columns or principal components. Looking at the second row in the table, Proportion of Variance, the first PC accounts for about 43% of total variation & the second PC accounts for about 20%, with the rest of the PC's each accounting for about 9% or less. To use PCA, the threshold for Cumulative Proportion is usually at least 70%-80%, but this is just a guideline as there are no set rules. 


#### Scree-Plot

This plot visually represents the 'Proportion of Variance' from the previous table. Here it is easy to see the drop off after the second PC. This type of plot can be used to help decide how many PC's should be used/kept.

```{r, plot2, fig.height=6, fig.width=12}
par(col.axis="#8B814C",col.lab="#8B814C",col.main="#8B814C",col.sub="#8B814C",pch=20, col="#8B814C", bg="transparent")
screeplot(princomp(df), type="lines", bg="transparent", col="#8B814C", main="")
```

#### Biplot

In this plot we see the eigenvectors (points) & eigenvalues (arrows). PCA can only be interpreted in either the x direction (horizontal distances) or in the y direction (vertical distances), but not both. Although the first PC is about 43% of variance, there is no distinction between political ideology groups in the horizontal plane. But, we do see a pattern in the vertical direction, specifically obamabirth & sorosplot consisting of more conservative ideology, and iraqjews and truther911 with more liberal ideology. 


```{r, plot3, fig.height=8, fig.width=12}
autoplot(pc1,
  # AUTOPLOT OPTIONS
  data=conspiracy, 
  colour="PoliticalIdeology", 
  loadings=T, loadings.colour=alpha("#191970", 0.5), 
  loadings.label=T, loadings.label.colour="#191970", loadings.label.size=5, loadings.label.hjust=0) + 
  # CUSTOM COLORS FOR POLITICAL IDEOLOGY GROUPS
  scale_colour_manual(values = alpha(paletteer_d("rcartocolor::Temps"), 0.5)) +
  # GGPLOT THEME OPTIONS
  theme_bw() +
  theme(legend.key = element_rect(fill = "#FAFAF5"),
        legend.background = element_rect(fill = "#FAFAF5"),
        legend.text = element_text(color = "#8B814C", size = 14),
        legend.title = element_text(color = "#8B814C", size = 16),
        panel.border = element_rect(color = "#8B814C"),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C", size = 14),
        axis.title = element_text(color = "#8B814C", size = 16),
        axis.ticks = element_line(color = "#8B814C"))
```

#### Loadings

The matrix of variable loadings (i.e., a matrix whose columns contain the eigenvectors).

```{r}
(princomp(df))$loadings
```

## Conclusion


PCA is a very powerful tool for data analysis. It allows us to see patterns and trends hidden in large sets of data that would otherwise be very difficult to make out. PCA is also very flexible in its application which is why it sees use in so many different fields. This flexibility has also led to many different adaptations of the method, each with its own variations added to adapt to the task at hand. Large data sets are extremely common amongst many different fields of study. Principal component analysis requires no assumption of a distribution and can be applied to many types of numerical data. We focus on the Assumptions we ensure the variables are continuous, intervals or ratio levels of measurements are also of the same scale and units.
We also make sure in our dataset that the variables are linearly related, and we use the scatterplot to check the variables. Lastly, we ensure Outliers are removed and those Null values that are missing are noted and removed.
In our conclusion of Data, we saw that Scree-plot Proportion of Variance 0.4256 0.2027 0.09081 0.0669 0.06306 0.05966 0.05214 0.03907 and as well that plot dropped 10% from variances after Comp 3. 
Lastly we noticed that the PCA can CA can only be interpreted in either the x direction (horizontal distances) or in the y direction (vertical distances), but not both but not diagonally 

PCA can map the principal components to a 2-D plane and create a cluster we can visualize and use to analyze the data set. We can also just focus on the principal components themselves and draw conclusions from them when visualization is not a valid option. PCA’s strength is that it retains as much variance in the data as possible while increasing the interpretability of the data and while it is sensitive to scaling it remains a very useful method of analysis. PCA has been found to be useful when performing k-means clustering which by itself is a clustering method with countless applications. With all of the flexibility and usefulness of PCA taken into consideration it is easy to see why it is such a popular way to analyze large data sets across so many fields. As long as there are large data sets there will be a demand to reduce the dimension of that data and make valid interpretations about the trends present in that data, to this end we have PCA.
 [@Richardson]

## References {.bibliography style="background-color: #FAFAF5; color: #8B814C"}
