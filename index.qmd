---
title: "PCA (principal component analysis)"
author: 
- Brandy Carr
- Lee
- Tavish
date: '`r Sys.Date()`'
format:
  html:
    highlight: pygments
    anchor-sections: false
    toc: true
    mainfont: sans-serif
    fontcolor: "#8B814C"
    monobackgroundcolor: "#FAFAF5"
    backgroundcolor: "#FAFAF5"
    code-line-numbers: true
    code-block-bg: "FFFFFF"
    code-fold: true
    code-background: true
    cap-location: bottom
    margin-left: "0in"
    margin-right: "0in"
    fig-width: 12
    fig-height: 8
execute: 
  warning: false
  echo: true
course: STA 6257 - Advance Statistical Modeling
bibliography: references.bib # file contains bibtex for references
always_allow_html: true # this allows to get PDF with HTML features
editor: 
  markdown:
    wrap: 100
---

```{=html}
<style>
details > summary {
  padding: 1px;
  color: #8B814C;
}
 .title {
    color: #191970;    
  }
 .subtitle {
    color: #191970;
  }
  .author {
    color: #191970;
}
 body {
    color: #8B814C;
    background-color: #FAFAF5;
  }
 .bibliography {
    background-color: #FAFAF5;
}
 .reference-section-title {
    color: #191970;
}
</style>
```
## [**1. Introduction**]{style="color: #191970;"}

[Principal Component Analysis and Machine Learning Methods. In our
research, we learn the applications of how important and groundbreaking
Principal Component Analysis and Machine Learning Methods are. In these
papers we see how promising the impact is of PCA with recognizing
Microanerysm in medicine and how groundbreaking has been critical for
diagnosis and treatment of Diabetic Retinopathy. Other research papers
show a framework for coronary artery disease risk assessment in
intravascular ultrasound. The paper reflects on a novel strategy for
risk stratification based on plaque morphology embedded with principal
component analysis (PCA) for plaque feature dimensionality reduction and
dominant feature selection technique.[@Gorg2021] Facial recognition with
PCA and machine learning methods, From this Paper the authors have
identified that the best way to compare and evaluate the Facial
Recognition results with speed and accuracy was with PCA (Principal
Component Analysis) and as well as Support Vector Machine (SVM). Lastly
we see the importance of PCA with it identifying that the best way to
compare and evaluate the Facial Recognition results with speed and
accuracy was with PCA (Principal Component Analysis) and as well as
Support Vector Machine (SVM)[@Camargo].]{style="color: ;"}

[Principal Component Analysis (PCA) is the name for a process of
reducing the dimension of a large data set (when the observations one
wishes to compare is described by many variables) into a smaller
simplified version of the original data set that retains the most
salient features of the data. PCA is a broadly used statistical method
whose use stretches across many scientific disciplines, and in turn many
different adaptations of PCA have been developed based on the variation
in goals and data types associated with each respective discipline. PCA
was developed first by Pearson (1901) and Hotelling (1933) [@Camargo].
This technique transforms some number of possibly correlated variables
into a smaller number of variables, the variables in this smaller matrix
are referred to as the Principal Components (PC). This is achieved using
a vector space transform. By mathematical projection we can interpret
the original data set with just a few variables, namely the principal
components achieved through calculation. The purpose of reducing
dimension size in large data sets is to make it easier to spot trends,
patterns, and outliers in data where that information would have
previously been hidden by the size of data set of interest (Richardson).
The information being preserved in the process of reduction is the
variability in the data (i.e. the statistical information present in the
data). In order to preserve as much variability as possible we should
"...find new variables that are linear functions of those in the
original data set, that successively maximize variance and that are
uncorrelated with each other" [@Jolliffe]. PCA assumes no distribution
in data in a descriptive context which is one of it's key features that
makes it so widely adaptive as an exploratory method across disciplines
and data types [@Jolliffe]. To lists all of PCA's applications would be
tedious and excessive, but some examples where PCA has been used is in
facial recognition, image analysis, analysis of web data, and cyber
security analysis. Essentially anywhere that large data sets are found
PCA can be used to aid in discovering trends amongst the variables of
that data. One key field PCA has seen use is in the analysis of cyber
security network data. PCA could see many applications in web data
analysis as the amount of data for this purpose is always increasing and
becoming more relevant.]{style="color: #8B814C;"}

[Taking a look into a real world example, say we have a dataset
consisting of 1,000 students with exam scores from 7 different courses:
Advanced Statistics, Probability Theory, Intro to Dance, World
Religions, and Religion in America. We could group Advanced Statistics
and Probability Theory into a new variable called Stats, group World
Religions and Religion in America into a new variable Religion, and keep
Intro to Dance by itself. We have Reduced the data set from 7 variables
to 3 without much loss in variation. This is the main concept behind PCA
except variables are not manually regrouped but instead the new
variables (principal components) are derived from certain linear
combinations of the original variables
[@lang2021applied].]{style="color: #8B814C;"}

[PCA can also be useful when studying mental disorders, data consisting
of symptoms and the connections between them. When many symptoms are
being observed, it can be difficult to visually represent the
connections between them, both the strength of the connections and the
proximity to each other. Plotting using the first 2 principal components
allows for the placement on the x or y axis to become interpretable,
that is, observations far left differ in some dimension (the first
principal component) compared to ones far right. The same can be said in
the y direction [@Jones].]{style="color: #8B814C;"}

![](index_files/figure-html/machineLearning_fig1.png)

## [**2. Methods**]{style="color: #191970;"}

[How PCA is used PCA is a tool for identifying the main axes of variance
within a data set and allows for easy data exploration to understand the
key variables in the data and spot outliers. Properly applied, it is one
of the most powerful tools in the data analysis tool kit. PCA forms the
basis of multivariate data analysis based on projection methods. The
most important use of PCA is to represent a multivariate data table as
smaller set of variables (summary indices) to observe trends, jumps,
clusters and outliers. This overview may uncover the relationships
between observations and variables, and among the variables.
Statistically, PCA finds lines, planes and hyper-planes in the
K-dimensional space that approximate the data as well as possible in the
least squares sense. A line or plane that is the least squares
approximation of a set of data points makes the variance of the
coordinates on the line or plane as large as
possible.]{style="color: #8B814C;"}

<br>


[**Sample Data:**]{style="color: #8B814C; font-size: 22px;"}

<style type="text/css">
.tg  {border-collapse:collapse;border-spacing:0;}
.tg td{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  overflow:hidden;padding:10px 8px;word-break:normal;}
.tg th{border-color:black;border-style:solid;border-width:1px;font-family:Arial, sans-serif;font-size:16px;
  font-weight:normal;overflow:hidden;padding:10px 8px;word-break:normal;}
.tg .tg-vi6l{background-color:#ffffff;border-color:#8b814c;color:#8b814c;text-align:center;vertical-align:top}
.tg .tg-pi0b{background-color:#eaead6;border-color:#8b814c;color:#8b814c;font-weight:bold;text-align:center;vertical-align:top}
</style>
<table class="tg" style="undefined;table-layout: fixed; width: 791px">
<colgroup>
<col style="width: 126px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
<col style="width: 111px">
</colgroup>
<thead>
  <tr>
    <th class="tg-pi0b"></th>
    <th class="tg-pi0b">$X_1$</th>
    <th class="tg-pi0b">$X_2$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_j$</th>
    <th class="tg-pi0b">...</th>
    <th class="tg-pi0b">$X_p$</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td class="tg-pi0b">$x_1$</td>
    <td class="tg-vi6l">$x_{11}$</td>
    <td class="tg-vi6l">$x_{12}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{1p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_2$</td>
    <td class="tg-vi6l">$x_{21}$</td>
    <td class="tg-vi6l">$x_{22}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2j}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{2p}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_j$</td>
    <td class="tg-vi6l">$x_{i1}$</td>
    <td class="tg-vi6l">$x_{i2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ij}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{ip}$</td>
  </tr>
  <tr>
    <td class="tg-pi0b">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">...</td>
  </tr>
  <tr>
    <td class="tg-pi0b">$x_n$</td>
    <td class="tg-vi6l">$x_{n1}$</td>
    <td class="tg-vi6l">$x_{n2}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{nj}$</td>
    <td class="tg-vi6l">...</td>
    <td class="tg-vi6l">$x_{np}$</td>
  </tr>
</tbody>
</table>

::: {.callout-note collapse="true"}

[No assumptions about the variable distributions needed]{style="color: #1A79E1; font-size:12px;"}

:::


[**Random Vector:**]{style="color: #8B814C; font-size: 22px;"}

[${x_i} = {({x_{i1}}, ... , {x_{ip}})}^T,\ \ \ \ \ \ \ \ \ where\ \ i\ =\ \{1,2,...,n\}$]{style="color: #8B814C;"}


[**Estimates for the Sample Mean Vector & Sample Covariance Matrix:**]{style="color: #8B814C; font-size: 22px;"}

[$\hat{\mu}\ \ =\ \ \overline{x}\ \ =\ \ \frac{1}{n} \sum_{i=1}^{n} x_i$]{style="color: #8B814C;"}

[$\hat{\sum}\ \ =\ \ S\ \ =\ \ {(\hat{\sigma}_{ij})}_{p{\sf x}p}\ \ =\ \ \frac{1}{n-1} \sum_{i=1}^{n} {(x_i - \overline{x})(x_i - \overline{x})}^T$]{style="color: #8B814C;"}

::: {.callout-note collapse="true"}

[The sample mean vector represents the centers of the random vectors x]{style="color: #1A79E1; font-size:12px;"}

[The sample covariance matrix represents the variations (diagonal elements) & correlations (off-diagonal elements) of the random vector x]{style="color: #1A79E1; font-size:12px;"}

:::

[**Principal Components:**]{style="color: #8B814C; font-size: 22px;"}

[${\hat{a}_{k}}$'s are the eigenvectors of the sample covariance matrix **S** (or the sample correlation matrix **R**)]{style="color: #8B814C;"}

[${\hat{y}_{ik}}\ =\ {\hat{a}^T_{k}}(x_i\ -\ \overline{x}),\ \ \ \ \ \ \ i\ =\ \{1,2,...,n\},\ \ \ \ \ \ \ k\ =\ \{1,2,...,p\}$]{style="color: #8B814C;"}

<br>

[PCA results depend on the scale or units of the variables so, for
unscaled data, calculations should either be preformed on the
correlation matrix (instead of the covariance matrix) or the data should
be standardized with mean 0 and variance 1 (z scores).
[@lang2021applied]]{style="color: #8B814C;"}

<br>


```{mermaid, code-fold: hide}

%%{init: {'theme': 'base', 'themeVariables': { 'background': '#FAFAF5', 'primaryColor': '#EAEAD6', 'nodeBorder': '#8B814C', 'lineColor': '#8B814C', 'primaryTextColor': '#191970', 'textColor': '#191970', 'fontSize': '12px', 'width': '100%'}}}%%

flowchart LR
A[(Clean<br/>Data)] --> B(Est<br/>Sample<br/>Mean<br/>Vector)
B --> C((Are all<br/>vars the same<br/>same scale/unit?))
C --- D((Yes))
C --- E((No))
D --> G(Est<br/>Sample<br/>Cov<br/>Matrix)
E -.- F(Stdz<br/>Data)
E --> H(Est<br/>Sample<br/>Corr<br/>Matrix)
F -.- G
G --> I(Eigenvals<br/>Eigenvecs)
H --> I

```

<br>

## [**3. Analysis and Results**]{style="color: #191970;"}

##### [Packages]{style="color: #191970;"}

```{r, packages}
library(dplyr)
library(ggplot2)
library(data.table)
library(ggfortify)
library(MASS)
library(tidyr)
library(paletteer)
library(knitr)
```

##### [Read Data]{style="color: #191970;"}

```{r}
# READ DATA FROM A GITHUB CSV FILE
conspiracy<- (read.csv("https://raw.githubusercontent.com/bjcarr08/sampleData/main/kaggleConspiracyTheoriesData.csv", stringsAsFactors = T))[,-1]

# VARIABLES
#kable(DescTools::Desc(conspiracy)[[1]]$abstract)
str(conspiracy)
```

##### [Clean Data]{style="color: #191970;"}

```{r}
# REMOVE ROWS WITH NAs & IMPOSSIBLE VALUES (removed rows where participant marked 'not sure' as political ideology)
conspiracy<- conspiracy[complete.cases(conspiracy),] %>% filter(y!="Not Sure")
```

##### [Visualize Distributions]{style="color: #191970;"}

```{r, plot1, fig.height=7, fig.width=12}
# TRANSFORM TO LONG DATA FOR PLOTS
conspiracy.Long<- conspiracy %>% pivot_longer(!y, names_to="conspiracy", values_to="score", values_transform=list(score=as.numeric))

# HISTOGRAMS
ggplot(conspiracy.Long, aes(score, fill=conspiracy, color=conspiracy)) +
  geom_histogram(alpha=0.2, breaks=seq(0,5,1)) +
  lemon::facet_rep_wrap(.~conspiracy, nrow=4, labeller="label_both", repeat.tick.labels=T) +
  labs(title="Raw Scores") +
  theme_bw() +
  theme(legend.position = "none",
        panel.border = element_rect(color = "#8B814C"),
        strip.background = element_rect(fill = "#EAEAD6", color = "#8B814C"),
        strip.text = element_text(color = "#8B814C"),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C"),
        axis.title = element_text(color = "#8B814C"),
        plot.title = element_text(color = "#8B814C"),
        axis.ticks = element_line(color = "#8B814C"))
```

##### [PCA]{style="color: #191970;"}

```{r}
options(width = 100)

# STANDARDIZE DATA
conspiracy.Stdz<- conspiracy %>% mutate(across(.cols=truther911:vaportrail, scale))

# RE-LEVEL POLITICAL IDEOLOGY (Very Liberal - Very Conservative)
conspiracy$y<- factor(conspiracy$y, levels=c("Very Liberal", "Liberal", "Somewhat Liberal", "Middle of the Road", "Somewhat Conservative", "Conservative", "Very Conservative"))

# RE-NAMED VARIABLE 'y'
names(conspiracy)[9]<- "PoliticalIdeology"

# DATA FOR PCA FUNCTION (only keep numeric variables)
df<- conspiracy[,-9]

# PCA
pc1<- prcomp(df, scale.=T)

summary(pc1)
```

##### [SCREE-PLOT]{style="color: #191970;"}

```{r}

```

##### [BIPLOT]{style="color: #191970;"}

```{r}
autoplot(pc1,
  # AUTOPLOT OPTIONS
  data=conspiracy, 
  colour="PoliticalIdeology", 
  loadings=T, loadings.colour=alpha("#191970", 0.5), 
  loadings.label=T, loadings.label.colour="#191970", loadings.label.size=5, loadings.label.hjust=0) + 
  # CUSTOM COLORS FOR POLITICAL IDEOLOGY GROUPS
  scale_colour_manual(values = alpha(paletteer_d("rcartocolor::Temps"), 0.5)) +
  # GGPLOT THEME OPTIONS
  theme_bw() +
  theme(legend.key = element_rect(fill = "#FAFAF5"),
        legend.background = element_rect(fill = "#FAFAF5"),
        legend.text = element_text(color = "#8B814C", size = 14),
        legend.title = element_text(color = "#8B814C", size = 16),
        panel.border = element_rect(color = "#8B814C"),
        plot.background = element_rect(fill = "#FAFAF5"),
        axis.text = element_text(color = "#8B814C", size = 14),
        axis.title = element_text(color = "#8B814C", size = 16),
        axis.ticks = element_line(color = "#8B814C"))
```

##### [LOADINGS]{style="color: #191970;"}

```{r}

```

## [Conclusion]{style="color: #191970;"}

## {.bibliography style="background-color: #FAFAF5; color: #8B814C"}
